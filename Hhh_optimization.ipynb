{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS and TENSORFLOW\n",
    "We start by importing all the necessary packages\n",
    "(%lsmagic for magic commanda and ! for bash instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I you had no error so far, this is great! We can start the tutorial.\n"
     ]
    }
   ],
   "source": [
    "import sys;\n",
    "import os, sys, array, re, math, random, subprocess, glob\n",
    "from math import *\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy.lib.recfunctions import stack_arrays\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Highway, MaxoutDense, Masking, GRU, Merge, Input, merge\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import deepdish.io as io\n",
    "import ROOT\n",
    "from ROOT import gSystem, gROOT, gApplication, TFile, TTree, TCut, TH1F, TCanvas\n",
    "from root_numpy import root2array \n",
    "from IPython.display import HTML, IFrame\n",
    "import seaborn as sns; sns.set()\n",
    "print \"I you had no error so far, this is great! We can start the tutorial.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set some variables we will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check this file out, it contains many functions we will use.\n",
    "execfile(\"Useful_func.py\")\n",
    "# Fix random seed for reproducibility\n",
    "seed = 7; np.random.seed(seed);\n",
    "# Input paramters\n",
    "debug = True #(Verbose output)\n",
    "folder='Plots_hh_tt_MLP/' # Folder with Plots\n",
    "MakePlots=False # Set False if you want to run faster\n",
    "folderCreation  = subprocess.Popen(['mkdir -p ' + folder], stdout=subprocess.PIPE, shell=True); folderCreation.communicate()\n",
    "folderCreation2 = subprocess.Popen(['mkdir -p models/'], stdout=subprocess.PIPE, shell=True); folderCreation2.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now We Start manipulating ROOT files into a format we can use to train a MVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> hh Displayed as panda dataframe: \n",
      "      mass_trans  dphi_llmet  dphi_llbb  eta_l1l2     pt_l1l2  mass_l1l2  \\\n",
      "0      72.777069   -0.886775   3.002455  0.922840   84.128227  29.190264   \n",
      "1     159.240128   -1.983317   2.945254 -0.149637  105.026260  44.730267   \n",
      "2     118.480865    1.224844  -1.547155 -1.470161  126.250465  24.067703   \n",
      "3      43.424339    0.340652  -3.017415  1.175856  114.969040  41.966232   \n",
      "4      33.620312    0.274080  -0.962814  1.154134   84.900856  63.884369   \n",
      "5     127.793579    1.449566   2.705430  1.900905  166.623047  18.158842   \n",
      "6      39.307190    0.292008  -2.213338 -0.131672  304.269775  36.601650   \n",
      "7      11.351856    0.127380   2.885273  1.712558   92.330399  18.176708   \n",
      "8      26.132015   -0.400537   2.394995  0.145146   72.556770  43.897713   \n",
      "9      49.478588   -0.587437   2.266979 -0.363785   46.964069  52.886749   \n",
      "10     96.429413   -1.294901   2.532745  1.689363   67.388756  12.660436   \n",
      "11     84.855293   -0.696022   2.843185  0.058142  132.675232  30.120659   \n",
      "12     60.607113    0.616687  -2.438187  1.960788   61.301426  30.372665   \n",
      "13     24.708801   -0.334658  -2.593711 -1.169025  143.842453  91.434158   \n",
      "14     80.816322   -2.976642  -0.773113 -0.817852   14.960975  46.714340   \n",
      "15     13.160537    0.116253  -2.863854 -0.566713  135.285583  39.682972   \n",
      "16     93.663651   -0.895869   3.057182 -0.638710   76.053581  37.658897   \n",
      "17     93.404228    0.490882  -2.717612 -0.805344  202.422455  35.568146   \n",
      "18     57.866688    0.450211  -1.777482  0.723801  134.440109  43.697567   \n",
      "19     14.283433    0.135136  -3.098888  2.131323   53.309772  65.066002   \n",
      "20     20.460733   -0.271058   3.103016  0.426864  119.429520  37.291153   \n",
      "21     71.767128    0.635164  -3.116910 -1.068657  191.456573  44.635811   \n",
      "22     68.340591    0.711831   2.468749 -0.336376   79.257217  36.535297   \n",
      "23     66.181366   -0.825357   1.983882 -1.177073   64.428520  46.680000   \n",
      "24     58.541191    0.987621  -2.899849 -1.202333  146.422485  20.829872   \n",
      "25     70.790268   -0.782133   2.381254 -1.056059  132.695389  19.561504   \n",
      "26    100.580551   -0.986430   2.492883 -1.441971  138.281647  41.341415   \n",
      "27    116.916809   -0.906306   2.485466  1.414883  179.808655  47.454414   \n",
      "28     83.249184   -1.101315   3.037702  0.425266  151.640747  48.819336   \n",
      "29     16.756706   -0.236456   2.842797  1.292473   53.535595  33.380409   \n",
      "...          ...         ...        ...       ...         ...        ...   \n",
      "7693   23.660490   -0.141393  -2.458165  0.371926  197.840805  43.175491   \n",
      "7694   79.755333   -0.908603  -3.021954  1.239793  142.352432  52.492039   \n",
      "7695   45.672878   -0.496946   2.705814  0.627419   50.158859  16.304419   \n",
      "7696   32.649586    1.183688   3.036475  0.804818   34.216438  47.829945   \n",
      "7697   11.659739   -0.191522   0.842333 -0.569583   90.663475  57.350525   \n",
      "7698   68.271492   -1.110859   2.133363 -1.315246   35.860748  21.158287   \n",
      "7699   45.978794    0.687751   3.120800  0.841442   96.341232  22.798960   \n",
      "7700   41.643204   -0.429002  -2.929747  1.342066   58.138710  41.282391   \n",
      "7701  113.458351   -1.081483  -2.918248  0.045078   76.079140  43.599716   \n",
      "7702   80.309586    2.086977  -1.894753  1.900169   53.183777  17.185106   \n",
      "7703   79.062416    0.700256   2.627441  0.487922   82.793716  41.223793   \n",
      "7704   84.141258   -1.388903   1.066919 -1.468628   52.348217  24.759014   \n",
      "7705   73.343826   -0.845822   2.721001 -2.038049   67.902405  21.392387   \n",
      "7706   64.834435   -0.631771   2.716346  0.859406   76.976006  36.740780   \n",
      "7707   74.793808    1.395685  -2.533950  0.152372  149.581818  25.103392   \n",
      "7708   41.220947    0.771835  -2.995872  0.819519   79.476898  29.253534   \n",
      "7709  123.682266    1.058154  -2.858602 -1.138626  158.090195  63.900784   \n",
      "7710   97.486382   -0.784921  -2.953897 -1.113449  108.936264  47.749874   \n",
      "7711  108.524391    1.007333  -2.884325 -0.283204   94.182190  49.963223   \n",
      "7712   35.182381    0.816139  -2.706929 -0.976470   95.821678  33.040657   \n",
      "7713   41.631672    0.531982   2.950458 -0.283340   95.196060  50.451469   \n",
      "7714   36.289795   -0.692418   2.659649 -2.167089   30.694498  48.994297   \n",
      "7715   37.722019   -0.231076  -2.637079 -0.019519  151.258011  41.195324   \n",
      "7716   46.183670   -0.494184  -2.448541  0.781559   34.273792  16.457067   \n",
      "7717  116.402649    1.272722  -2.330137  0.209420   93.383034  19.859331   \n",
      "7718   39.705997   -0.353970  -2.641164  1.611468  122.471870  13.292072   \n",
      "7719   43.534718   -0.245244   0.119419 -0.113505  140.067413  13.278055   \n",
      "7720   34.109890    0.448742  -2.948067 -0.918819   44.046654  35.254070   \n",
      "7721   97.103027   -1.890782  -1.720567  1.496989   46.872952  49.714600   \n",
      "7722   48.662209    0.804458  -2.389483 -0.808898   22.666161  26.477287   \n",
      "\n",
      "      eta_b1b2     pt_b1b2   mass_b1b2  dR_minbl  dR_l1l2b1b2           HT  \\\n",
      "0     0.679283  337.605499  131.521683  2.377240     3.012317   909.700012   \n",
      "1    -2.215597  125.428627   97.832443  2.475216     3.597599   333.473541   \n",
      "2    -0.433335  231.515625  135.355820  1.661638     1.862444   713.107605   \n",
      "3    -0.500419  195.556839  468.009369  2.367721     3.451766   878.605896   \n",
      "4     1.420563  431.784576  124.797218  1.047928     0.998997  1240.483154   \n",
      "5     2.229145  129.435349   75.028038  2.004860     2.725269   509.756958   \n",
      "6    -1.987973   66.954224  113.363335  2.159484     2.888722  1050.191040   \n",
      "7     0.791380   99.414101  167.495132  2.459890     3.028756   640.250488   \n",
      "8     1.059831  181.122955  111.920738  1.846238     2.563718   487.097290   \n",
      "9    -0.631312  191.268204  109.334229  1.684937     2.282709   485.440796   \n",
      "10    1.131564  182.399445  116.235588  1.964762     2.593441   382.412109   \n",
      "11   -0.770918  196.950516  122.887917  2.431607     2.961594   486.575806   \n",
      "12    1.860683  197.145187  118.901619  1.967372     2.440241   557.977844   \n",
      "13   -0.387810  239.716904  140.504868  2.213821     2.708807   607.514954   \n",
      "14    1.897450   70.075043   91.072723  1.373876     2.823219   296.768585   \n",
      "15   -1.763598  162.479797  122.251060  2.417101     3.103900   491.688995   \n",
      "16   -1.500456  211.957458  127.947418  1.900982     3.176314   492.440369   \n",
      "17   -1.430061  177.876770  142.218246  2.707891     2.788491   660.167725   \n",
      "18    1.291460  172.308105  132.000458  0.665370     1.865926   906.536072   \n",
      "19    2.142467  201.991638  135.977142  1.458612     3.098908   402.326233   \n",
      "20   -0.325769  207.921265   51.909649  2.617446     3.192987   664.180176   \n",
      "21   -1.803403  165.684326  151.011475  2.274291     3.202339   509.941162   \n",
      "22   -1.141451  249.620285  125.047249  2.208657     2.596703   613.462402   \n",
      "23    1.183857  106.049332  192.289001  1.361152     3.083793   474.218933   \n",
      "24   -0.247497  156.118668  121.065918  1.970305     3.053005   720.977783   \n",
      "25   -0.882998   64.111008   81.616211  1.902692     2.387535   488.202301   \n",
      "26   -0.624073  175.560272  113.071930  0.922444     2.623628   499.422821   \n",
      "27    0.657543   85.294258   76.229416  2.116761     2.598289   537.679932   \n",
      "28   -1.208894  138.517700  113.175812  2.208863     3.449364   427.509613   \n",
      "29    1.665493  147.388306   82.737419  2.359035     2.867165   304.065308   \n",
      "...        ...         ...         ...       ...          ...          ...   \n",
      "7693  0.230657  121.722206  102.987495  2.112569     2.462221   723.208252   \n",
      "7694 -0.405012  115.945541  121.954544  2.508372     3.440580   616.498840   \n",
      "7695  2.057164  157.417358  101.109169  1.965732     3.060326   316.148438   \n",
      "7696 -2.585130   63.051952  124.281914  1.792642     4.551036   598.310852   \n",
      "7697  1.200079  118.476517   65.472130  1.301365     1.959905   542.806580   \n",
      "7698  0.420508  153.280334  124.244270  2.501360     2.750287   373.816010   \n",
      "7699 -1.268072  176.316971  132.842331  3.126994     3.766888   422.471405   \n",
      "7700  2.298552  210.486298  104.756050  2.155229     3.081928  1166.468872   \n",
      "7701 -1.654149  113.885658   96.705780  2.058202     3.376913   449.072693   \n",
      "7702 -0.931429   11.942322  120.711494  2.189700     3.407057   196.080490   \n",
      "7703  0.040889  252.108948  145.769318  1.529784     2.665199  1082.674072   \n",
      "7704  1.664091   22.012194   73.005272  2.168132     3.309417   570.992371   \n",
      "7705 -0.862483  146.374893  103.757744  2.102015     2.964086   294.325317   \n",
      "7706  0.595616  208.403564  136.495132  2.054530     2.729124   505.976044   \n",
      "7707  1.643500  111.788452   90.778183  2.080473     2.940130   582.539856   \n",
      "7708  2.564837  102.369370  108.635551  1.620668     3.467187   382.246674   \n",
      "7709 -1.401896  249.927521  134.286514  1.705883     2.870699   702.501831   \n",
      "7710 -0.063730  250.745316  206.507416  2.672925     3.134871   555.857422   \n",
      "7711  0.036547  199.954178  107.651489  2.270091     2.901994   512.064331   \n",
      "7712  0.754530   63.708733   66.950279  2.586416     3.213071   350.179688   \n",
      "7713 -1.420703  219.423828  124.841438  2.531297     3.162088   746.648254   \n",
      "7714  1.052083   31.250540  134.763489  0.701743     4.175740   328.770142   \n",
      "7715  1.012130  154.749924  179.057404  2.325963     2.831693   608.554504   \n",
      "7716  1.644239  122.643204   92.764252  2.301133     2.596068   640.396606   \n",
      "7717  0.402547  264.013153  129.717529  1.541088     2.338127   613.469421   \n",
      "7718  1.045265  146.566620  110.397446  2.218249     2.701173   577.524536   \n",
      "7719  1.434060  128.524033  105.836655  1.028137     1.552166   917.065308   \n",
      "7720 -1.206960  124.558167   67.260719  1.561057     2.962115   344.437164   \n",
      "7721  1.096830  383.167664  109.304993  1.341535     1.766487  1223.962524   \n",
      "7722 -1.021945  136.926865   73.933075  1.503957     2.398962   297.939362   \n",
      "\n",
      "          met_pt  muon1_pogSF  muon2_pogSF  XsecBr  fin_weight  \n",
      "0      85.520317     0.968178     0.969689     1.0    0.938832  \n",
      "1      86.171532     0.976864     0.969454     1.0    0.947025  \n",
      "2      84.118759     0.987606     0.980503     1.0    0.968351  \n",
      "3     142.714340     0.947636     0.977923     1.0    0.926715  \n",
      "4     178.343628     0.967422     0.980437     1.0    0.948496  \n",
      "5      55.748238     0.999864     0.975164     1.0    0.975031  \n",
      "6      59.976871     0.971556     0.976632     1.0    0.948852  \n",
      "7      86.133476     0.986038     0.962405     1.0    0.948969  \n",
      "8      59.456017     0.965680     0.997253     1.0    0.963028  \n",
      "9     155.478897     0.967345     0.979122     1.0    0.947149  \n",
      "10     94.823082     0.982158     0.994981     1.0    0.977228  \n",
      "11    116.661209     0.976724     0.968379     1.0    0.945839  \n",
      "12    162.650391     0.978143     0.997691     1.0    0.975884  \n",
      "13     38.253399     0.946668     0.977210     1.0    0.925093  \n",
      "14    109.884323     0.980195     0.996412     1.0    0.976678  \n",
      "15     94.836807     0.976914     0.966203     1.0    0.943897  \n",
      "16    153.736481     0.966391     0.959602     1.0    0.927351  \n",
      "17    182.498169     0.969112     0.979214     1.0    0.948968  \n",
      "18    124.980972     0.970121     0.996470     1.0    0.966696  \n",
      "19    209.884491     0.982206     0.975314     1.0    0.957959  \n",
      "20     48.002724     0.978199     0.970129     1.0    0.948979  \n",
      "21     68.969841     0.937798     0.978765     1.0    0.917884  \n",
      "22    121.333679     0.976926     0.983504     1.0    0.960810  \n",
      "23    105.658615     0.981718     0.969244     1.0    0.951525  \n",
      "24     26.045141     0.946800     0.987747     1.0    0.935199  \n",
      "25     64.980522     0.946949     0.948193     1.0    0.897890  \n",
      "26     81.589912     0.987220     0.947097     1.0    0.934993  \n",
      "27     99.157761     1.001047     0.998572     1.0    0.999618  \n",
      "28     41.732018     0.970222     0.959023     1.0    0.930466  \n",
      "29     94.244637     0.980624     0.998524     1.0    0.979176  \n",
      "...          ...          ...          ...     ...         ...  \n",
      "7693  141.775162     0.970232     0.968678     1.0    0.939843  \n",
      "7694   58.008663     0.947494     0.980446     1.0    0.928967  \n",
      "7695  171.912094     0.970222     0.997798     1.0    0.968086  \n",
      "7696   25.024181     0.969950     0.960455     1.0    0.931593  \n",
      "7697   41.004803     0.966408     0.966408     1.0    0.933944  \n",
      "7698  116.861183     0.984123     0.994012     1.0    0.978230  \n",
      "7699   48.264378     0.948763     0.968573     1.0    0.918947  \n",
      "7700  164.579086     0.980600     0.960396     1.0    0.941765  \n",
      "7701  159.631271     0.967361     0.970162     1.0    0.938497  \n",
      "7702   40.597782     0.980142     0.994262     1.0    0.974518  \n",
      "7703  160.416245     0.968325     0.968744     1.0    0.938059  \n",
      "7704   82.555252     0.980958     0.977379     1.0    0.958768  \n",
      "7705  117.579605     0.980050     0.994908     1.0    0.975059  \n",
      "7706  141.459045     0.968392     0.968319     1.0    0.937712  \n",
      "7707   22.644190     0.977553     0.967049     1.0    0.945342  \n",
      "7708   37.723690     0.978252     0.995626     1.0    0.973973  \n",
      "7709   94.955544     0.987638     0.977274     1.0    0.965193  \n",
      "7710  149.099701     0.947060     0.982105     1.0    0.930113  \n",
      "7711  134.208420     0.977367     0.980225     1.0    0.958039  \n",
      "7712   20.506901     0.946979     0.969149     1.0    0.917764  \n",
      "7713   65.872040     0.977364     0.960114     1.0    0.938381  \n",
      "7714   93.152275     0.986380     0.992255     1.0    0.978741  \n",
      "7715  176.968628     0.968791     0.978665     1.0    0.948123  \n",
      "7716  260.071747     0.980038     0.998227     1.0    0.978301  \n",
      "7717  102.713196     0.977269     0.979814     1.0    0.957541  \n",
      "7718  103.820084     0.986259     0.984956     1.0    0.971422  \n",
      "7719  226.107239     0.976662     0.983985     1.0    0.961021  \n",
      "7720  133.399734     0.967674     0.997251     1.0    0.965014  \n",
      "7721   76.512962     0.983057     0.998396     1.0    0.981481  \n",
      "7722  170.430801     0.977425     0.997290     1.0    0.974776  \n",
      "\n",
      "[7723 rows x 17 columns]\n",
      "The shape for hh is (samples, features): \n",
      "(7723, 17)\n",
      "The shape for tt is (samples, features): \n",
      "(1891414, 19)\n"
     ]
    }
   ],
   "source": [
    "# Our goal is to separate BX Signal from TT background. The machine learning only want to analyze \"good events\": \n",
    "# 1) Applying a preselection to out Signal and background events\n",
    "my_selec = 'met_pt>20 && met_pt<500 && muon1_pt>20 && fabs(muon1_eta)<2.4 && muon2_pt>10 && fabs(muon2_eta)<2.4 && pt_l1l2<500 && pt_b1b2<500 && mass_l1l2>12 && mass_l1l2<500 && mass_b1b2<500 && b1jet_pt>20 && fabs(b1jet_eta)<2.4 && b2jet_pt>20 && fabs(b2jet_eta)<2.4 && mass_trans>10 && mass_trans<500 && HT<4000'\n",
    "# 2) Selecting the branches that contains the information we want to use (in general)\n",
    "my_branches = [\"mass_trans\",\"dphi_llmet\",\"dphi_llbb\",\"eta_l1l2\",\"pt_l1l2\",\"mass_l1l2\",\"eta_b1b2\",\"pt_b1b2\",\"mass_b1b2\",\"dR_minbl\",\"dR_l1l2b1b2\",\"HT\",\"met_pt\",\"muon1_pogSF\",\"muon2_pogSF\",\"XsecBr\"]\n",
    "# 3) Selecting the branches that contains the information we want to use (in the training)\n",
    "my_branches_training = [\"mass_trans\",\"dphi_llmet\",\"dphi_llbb\",\"eta_l1l2\",\"pt_l1l2\",\"mass_l1l2\",\"eta_b1b2\",\"pt_b1b2\",\"mass_b1b2\",\"dR_minbl\",\"dR_l1l2b1b2\",\"HT\",\"met_pt\"]\n",
    "    \n",
    "# Converting Root files in dataframe (Very useful, checnl root2panda in Useful_func.py)\n",
    "hh    = root2panda('files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-500_narrow_13TeV-madgraph-v2.root', 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "## TT is so heavy that I saved the dataframe (lighter) before. You can load directly the csv. Open it just for curiousity with vim/emacs.\n",
    "allTT_files = ['files/tt_dataframe_0.csv','files/tt_dataframe_1.csv','files/tt_dataframe_2.csv','files/tt_dataframe_3.csv','files/tt_dataframe_4.csv']\n",
    "ttbar_s = (pd.read_csv(f) for f in allTT_files)\n",
    "ttbar   = pd.concat(ttbar_s, ignore_index=True)                    \n",
    "#ttbar = pd.read_csv(\"files/tt_dataframe.csv\")\n",
    "#ttbar_splitted = np.array_split(ttbar, 5)\n",
    "#ttbar_splitted[0].to_csv('files/tt_dataframe_0.csv')\n",
    "#ttbar_splitted[1].to_csv('files/tt_dataframe_1.csv')\n",
    "#ttbar_splitted[2].to_csv('files/tt_dataframe_2.csv')\n",
    "#ttbar_splitted[3].to_csv('files/tt_dataframe_3.csv')\n",
    "#ttbar_splitted[4].to_csv('files/tt_dataframe_4.csv')\n",
    "# Create a variable that is the total weight (weight=weight for xsec, reweighting=weight depending on muons)\n",
    "hh['fin_weight']    = hh['XsecBr'] * hh['muon1_pogSF'] * hh['muon2_pogSF'] #1pb\n",
    "ttbar['fin_weight'] = ttbar['XsecBr'] * ttbar['muon1_pogSF'] * ttbar['muon2_pogSF'] #87pb\n",
    "    ## Save the dataframe as h5 file (for quick loading in the future).\n",
    "#io.save(open('models/ttbar.h5', 'wb'), ttbar)\n",
    "#ttbar = io.load(open('models/ttbar.h5', 'rb'))\n",
    "\n",
    "if debug:\n",
    "  print(\"---> hh Displayed as panda dataframe: \"); print(hh)\n",
    "  print(\"The shape for hh is (samples, features): \"); print(hh.shape)\n",
    "  print(\"The shape for tt is (samples, features): \"); print(ttbar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Plots of the branches we selected\n",
    "if MakePlots:\n",
    "  for key in ttbar.keys() :\n",
    "      if(key!=\"muon1_pogSF\" and key!=\"muon2_pogSF\" and key!=\"XsecBr\" and key!=\"fin_weight\") :\n",
    "        matplotlib.rcParams.update({'font.size': 16})\n",
    "        fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "        bins = np.linspace(my_max(min(ttbar[key]),0.), max(ttbar[key]), 50)\n",
    "        _ = plt.hist(hh[key],  bins=bins, histtype='step', normed=True, label=r'$hh$', linewidth=2)\n",
    "        _ = plt.hist(ttbar[key], bins=bins, histtype='step', normed=True, label=r'$t\\overline{t}$')\n",
    "        plt.xlabel(key)\n",
    "        plt.ylabel('Entries')\n",
    "        plt.legend(loc='best')\n",
    "        print('Saving:',folder + '/' + str(key) + '.pdf')\n",
    "        plt.savefig(folder + \"/\" + str(key) + '.pdf')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets look at the correlations of the features\n",
    "c1 = ROOT.TCanvas(); c1.cd(); ROOT.gStyle.SetOptStat(0)\n",
    "if MakePlots:\n",
    "    h_Corr_hh    = ROOT.TH2F(\"h_Corr_hh\",\"\", len(my_branches_training), 0, len(my_branches_training), len(my_branches_training), 0, len(my_branches_training))\n",
    "    h_Corr_ttbar = ROOT.TH2F(\"h_Corr_ttbar\",\"\", len(my_branches_training), 0, len(my_branches_training), len(my_branches_training), 0, len(my_branches_training))\n",
    "    for var1 in range(len(my_branches_training)):\n",
    "        h_Corr_hh.GetXaxis().SetBinLabel(var1+1,my_branches_training[var1])\n",
    "        h_Corr_ttbar.GetXaxis().SetBinLabel(var1+1,my_branches_training[var1])\n",
    "        for var2 in range(len(my_branches_training)):\n",
    "            h_Corr_hh.GetYaxis().SetBinLabel(var2+1,my_branches_training[var2])\n",
    "            h_Corr_ttbar.GetYaxis().SetBinLabel(var2+1,my_branches_training[var2])\n",
    "            if(var2>=var1):\n",
    "                array_Var1_hh_var1    = np.array( hh[my_branches_training[var1]] )\n",
    "                array_Var1_hh_var2    = np.array( hh[my_branches_training[var2]] )\n",
    "                array_Var1_ttbar_var1 = np.array( ttbar[my_branches_training[var1]] )\n",
    "                array_Var1_ttbar_var2 = np.array( ttbar[my_branches_training[var2]] )\n",
    "                corr = scipy.stats.pearsonr( array_Var1_hh_var1, array_Var1_hh_var2 )[0]\n",
    "                h_Corr_hh.SetBinContent(var1+1,var2+1,corr)\n",
    "                corr = scipy.stats.pearsonr( array_Var1_ttbar_var1, array_Var1_ttbar_var2 )[0]\n",
    "                h_Corr_ttbar.SetBinContent(var1+1,var2+1,corr)\n",
    "    h_Corr_hh.GetZaxis().SetRangeUser(-1.,1.)\n",
    "    h_Corr_ttbar.GetZaxis().SetRangeUser(-1.,1.)\n",
    "    ROOT.gStyle.SetPaintTextFormat(\".2f\");\n",
    "    h_Corr_hh.Draw(\"colzTEXT\")\n",
    "    c1.SaveAs(folder + '/Corr_hh.pdf')\n",
    "    h_Corr_ttbar.Draw(\"colzTEXT\")\n",
    "    c1.SaveAs(folder + '/Corr_ttbar.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need:\n",
    "1. X : matrix with raw=#Events and column=Variables to discriminate.\"\n",
    "2. w : A vector containig the weights of each event\"\n",
    "3. Y : A vector containing for each event if it is signal (0) or TT (1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now lets start to talk about DNN!\n"
     ]
    }
   ],
   "source": [
    "print('Now lets start to talk about DNN!')\n",
    "#You only need a Dataframe for the training. So you merge all the one you have\n",
    "df =  pd.concat((hh[my_branches_training], ttbar[my_branches_training]), ignore_index=True)\n",
    "# Turn the df the desired ndarray \"X\" that can be directly used for ML applications.\n",
    "X = df.as_matrix() # Each row is an object to classify, each column corresponds to a specific variable.\n",
    "# Take the weights\n",
    "w =  pd.concat((hh['fin_weight'], ttbar['fin_weight']), ignore_index=True).values\n",
    "# This is the array with the true values: 0 is signal, 1 if TT.\n",
    "y = []\n",
    "for _df, ID in [(hh, 0), (ttbar, 1)]:\n",
    "    y.extend([ID] * _df.shape[0])\n",
    "y = np.array(y)\n",
    "\n",
    "# Randomly shuffle and automatically split all your objects into train and test subsets\n",
    "ix = range(X.shape[0]) # array of indices, just to keep track of them for safety reasons and future checks\n",
    "X_train, X_test, y_train, y_test, w_train, w_test, ix_train, ix_test = train_test_split(X, y, w, ix, train_size=0.7) # Train here is 70% of the total statistic\n",
    "# It is common practice to scale the inputs to Neural Nets such that they have approximately similar ranges (it atually improve the results)\n",
    "scaler = StandardScaler() \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This takes a while, but it is worth do do it once. It shows the correlations for S and B overimposed.\n",
    "# More variables you add in var_toPlot, more times it takes.\n",
    "if MakePlots:\n",
    "    %matplotlib inline\n",
    "    # Next lines to select the same number of entries\n",
    "    rawS = hh.shape[0]\n",
    "    rawB = ttbar.shape[0]\n",
    "    Nraw = rawS;\n",
    "    if (rawB<rawS): Nraw = rawB;\n",
    "    hh_plot    = hh.iloc[0:Nraw]\n",
    "    ttbar_plot = ttbar.iloc[0:Nraw]\n",
    "    # Add the target, that is missing in hh and ttbar\n",
    "    hh_plot['Target'] = 0\n",
    "    ttbar_plot['Target'] = 1\n",
    "    var_toPlot=[\"Target\",\"dphi_llmet\",\"pt_l1l2\",\"mass_l1l2\",\"pt_b1b2\",\"mass_b1b2\",\"HT\",\"met_pt\"]\n",
    "    #var_toPlot = [\"Target\",\"mass_b1b2\",\"mass_l1l2\"]\n",
    "    df_withY = pd.concat((hh_plot[var_toPlot], ttbar_plot[var_toPlot]), ignore_index=True)\n",
    "    # You can select the variable to plot in sns.pairplot using an argumnet vars=['var1','var2'...]\n",
    "    sns_plot = sns.pairplot(df_withY, hue='Target',palette=[\"#e74c3c\",\"#9b59b6\"],plot_kws={\"s\": 3,\"alpha\":0.3},size=5)\n",
    "    sns_plot.savefig(folder + \"/Variables_pairplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 1,252\n",
      "Trainable params: 1,252\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron (MLP) definition\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=X_train.shape[1], activation='relu')) # Linear transformation of the input vector. The first number is output_dim.\n",
    "model.add(Dropout(0.1)) # To avoid overfitting. It masks the outputs of the previous layer such that some of them will randomly become inactive and will not contribute to information propagation.\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20, activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10,activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20, activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='softmax')) # Last layer has to have the same dimensionality as the number of classes we want to predict, here 2.\n",
    "model.summary()\n",
    "# Now you need to declare what loss function and optimizer to use (and compile your model).\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('---------------------------Training:---------------------------')\n",
    "try:\n",
    "    model.fit(X_train, y_train, batch_size=50, epochs=100, verbose=1,\n",
    "              callbacks = [\n",
    "                  EarlyStopping(verbose=True, patience=6, monitor='val_loss'),\n",
    "                  ModelCheckpoint('models/tutorial-progress.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "              ],\n",
    "              validation_split=0.2, validation_data=None, shuffle=True,\n",
    "              class_weight={\n",
    "                0 : compute_class_weight(\"balanced\", [0, 1], y)[0], # Function that return \"[1/N_classes * ((float(len(y)) / (y == 0).sum())), 1/N_classes * ((float(len(y)) / (y == 1).sum()))]\"\n",
    "                1 : compute_class_weight(\"balanced\", [0, 1], y)[1]\n",
    "              },\n",
    "              sample_weight=None,initial_epoch=0)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print 'Training ended early.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best network (by default you return the last one, you if you save every time you have a better one you are fine loading it later)\n",
    "model.load_weights('./models/tutorial-progress.h5')\n",
    "print 'Saving weights...'\n",
    "model.save_weights('./models/tutorial.h5', overwrite=True)\n",
    "json_string = model.to_json()\n",
    "open('./models/tutorial.json', 'w').write(json_string)\n",
    "print 'Testing...'\n",
    "yhat = model.predict(X_test, verbose = True, batch_size = 50) # Return a vector of 2 indeces [probToBe_S,probToBe_B]\n",
    "#Turn them into classes\n",
    "yhat_cls = np.argmax(yhat, axis=1) # Transform [probToBe_S,probToBe_B] in a vector of 0 and 1 depending if probToBe_S>probToBe_B. Practically return the index of the biggest element (0 is is probToBe_S, if is probToBe_B)\n",
    "# This should Normalized to the Xsec?\n",
    "if MakePlots:\n",
    "    bins = np.linspace(-0.5,1.5,3)\n",
    "    names = ['','','','hh','','','','tt']\n",
    "    fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "    ax = plt.subplot()\n",
    "    ax.set_xticklabels(names, rotation=45)\n",
    "    _ = plt.hist(yhat_cls, bins=bins, histtype='stepfilled', alpha=0.5, label='prediction',log=True)#, weights=w_test)\n",
    "    _ = plt.hist(y_test, bins=bins, histtype='stepfilled', alpha=0.5, label='truth',log=True)#, weights=w_test)\n",
    "    plt.legend(loc='upper right')\n",
    "    print('Saving:',folder + '/Performance.pdf')\n",
    "    plt.savefig(folder + '/Performance.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With \"(y_test != 0) & (yhat_cls == 0)\" you get an arrate of bool: [False False False ..., False False False]\n",
    "print 'Signal efficiency:',     w_test[(y_test == 0) & (yhat_cls == 0)].sum() / w_test[y_test == 0].sum()\n",
    "print 'Background efficiency:', w_test[(y_test != 0) & (yhat_cls == 0)].sum() / w_test[y_test != 0].sum()\n",
    "w_1 = np.ones(len(y_test))\n",
    "print 'Signal efficiency (not weighted):',     w_1[(y_test == 0) & (yhat_cls == 0)].sum() / w_1[y_test == 0].sum()\n",
    "print 'Background efficiency (not weighted):', w_1[(y_test != 0) & (yhat_cls == 0)].sum() / w_1[y_test != 0].sum()\n",
    "print \"Let's compare with the training samples:\"\n",
    "w_1 = np.ones(len(y_train))\n",
    "yhat_tr = model.predict(X_train, verbose = True, batch_size = 50)\n",
    "yhat_trcls = np.argmax(yhat_tr, axis=1)\n",
    "print ''; print 'Signal efficiency (not weighted) for training:',     w_1[(y_train == 0) & (yhat_trcls == 0)].sum() / w_1[y_train == 0].sum()\n",
    "print 'Background efficiency (not weighted) for training:', w_1[(y_train != 0) & (yhat_trcls == 0)].sum() / w_1[y_train != 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
