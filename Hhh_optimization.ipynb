{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys;\n",
    "print 'We begin the tutorial importing several classes we will need.'\n",
    "import os, sys, array, re, math, random, subprocess, glob\n",
    "from math import *\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy.lib.recfunctions import stack_arrays\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Testing the IMPORT of sklearn, keras, and deepdish.'\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Highway, MaxoutDense, Masking, GRU, Merge, Input, merge\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import deepdish.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Testing the IMPORT of pyROOT.'\n",
    "import ROOT\n",
    "from ROOT import gSystem, gROOT, gApplication, TFile, TTree, TCut, TH1F, TCanvas\n",
    "from root_numpy import root2array, root2array \n",
    "print \"I you had no error so far, this is great! We can start the tutorial.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check this file out, it contains many functions we will use.\n",
    "execfile(\"Useful_func.py\")\n",
    "# Fix random seed for reproducibility\n",
    "seed = 7; np.random.seed(seed);\n",
    "# Input paramters\n",
    "debug = True #(Verbose output)\n",
    "folder='Plots_Hhh_tt_MLP/' # Folder with Plots\n",
    "MakePlots=True # Set False if you want to run faster\n",
    "folderCreation  = subprocess.Popen(['mkdir -p ' + folder], stdout=subprocess.PIPE, shell=True); folderCreation.communicate()\n",
    "folderCreation2 = subprocess.Popen(['mkdir -p models/'], stdout=subprocess.PIPE, shell=True); folderCreation2.communicate()\n",
    "\n",
    "# Pre-selection and branches to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our goal is to separate BX Signal from TT background. The machine learning only want to analyze \"good events\"\n",
    "# We defoe god events by: \n",
    "# 1) Applying a preselection to out Signal and background events\n",
    "my_selec = 'MMC_h2massweight1_prob>200 && hasRECOjet1 && hasRECOjet1 && hasMET && hastwomuons && (((b1jet_btag&2)>0 && (b2jet_btag&3)>0) || ((b1jet_btag&3)>0 && (b 2jet_btag&2)>0)) && dR_l1l2<3.3 && dR_l1l2>0.07 && dR_b1b2<5. && mass_l1l2<100 && mass_l1l2>5. && mass_b1b2>150 && dR_bl<5 && dR_l1l2b1b2<6 && MINdR_bl<3.2 && MINdR_bl>0.4 && mass_b1b2<700 && mass_trans<250 && MT2<400 && pt_b1b2<300'\n",
    "# 2) Selecting the branches that contains the information we want to use (in general)\n",
    "my_branches =  [\"dR_l1l2\",\"dR_b1b2\",\"dR_bl\",\"dR_l1l2b1b2\",\"MINdR_bl\",\"dphi_l1l2b1b2\",\"mass_l1l2\",\"mass_b1b2\",\"mass_trans\",\"MT2\",\"pt_b1b2\",\"weight\",\"reweighting\",\"MMC_h2massweight1_prob\"]\n",
    "# 3) Selecting the branches that contains the information we want to use (in the training)\n",
    "my_branches_training = [\"dR_l1l2\",\"dR_b1b2\",\"dR_bl\",\"dR_l1l2b1b2\",\"MINdR_bl\",\"dphi_l1l2b1b2\",\"mass_l1l2\",\"mass_b1b2\",\"mass_trans\",\"MT2\",\"pt_b1b2\",\"MMC_h2massweight1_prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Root files in dataframe (Very useful, checnl root2panda in Useful_func.py)\n",
    "Hhh3  = root2panda('files_root/delphes_B3_1M_PU40ALL_1Ag_mvammc_TRIMMED.root', 'evtree', branches=my_branches, selection=my_selec)\n",
    "ttbar = root2panda('files_root/delphes_tt_4M_PU40_WtomuALL_1Ag_mvammc_TRIMMED_5Kevents.root', 'evtree', branches=my_branches, selection=my_selec)\n",
    "# Create a variable that is the total weight (weight=weight for xsec, reweighting=weight depending on muons)\n",
    "Hhh3['fin_weight']  = Hhh3['weight']*Hhh3['reweighting']\n",
    "ttbar['fin_weight'] = ttbar['weight']*ttbar['reweighting']\n",
    "## Save the dataframe as h5 file (for quick loading in the future).\n",
    "#io.save(open('models/ttbar.h5', 'wb'), ttbar)\n",
    "#ttbar = io.load(open('models/ttbar.h5', 'rb'))\n",
    "\n",
    "if debug:\n",
    "  print(\"---> Hhh3 Displayed as panda dataframe: \"); print(Hhh3)\n",
    "  print(\"The shape for Hhh3 is [nb_events, nb_variables]: \"); print(Hhh3.shape)\n",
    "  print(\"The shape for tt is [nb_events, nb_variables]: \"); print(ttbar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of the branches we selected\n",
    "if (MakePlots) :\n",
    "  for key in ttbar.keys() :\n",
    "      if(key!=\"weight\" and key!=\"reweighting\" and key!=\"fin_weight\") :\n",
    "        matplotlib.rcParams.update({'font.size': 16})\n",
    "        fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "        bins = np.linspace(my_max(min(ttbar[key]),0.), max(ttbar[key]), 50)\n",
    "        _ = plt.hist(Hhh3[key],  bins=bins, histtype='step', normed=True, label=r'$Hhh B3$', linewidth=2)\n",
    "        _ = plt.hist(ttbar[key], bins=bins, histtype='step', normed=True, label=r'$t\\overline{t}$')\n",
    "        plt.xlabel(key)\n",
    "        plt.ylabel('Entries')\n",
    "        plt.legend(loc='best')\n",
    "        print('Saving:',folder + '/' + str(key) + '.png')\n",
    "        plt.savefig(folder + \"/\" + str(key) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Now lets start to talk about DNN!')\n",
    "# Not lets draw the correlations\n",
    "c1 = ROOT.TCanvas(); c1.cd(); ROOT.gStyle.SetOptStat(0)\n",
    "h_Corr_Hhh3  = ROOT.TH2F(\"h_Corr_Hhh3\",\"\", len(my_branches_training), 0, len(my_branches_training), len(my_branches_training), 0, len(my_branches_training))\n",
    "h_Corr_ttbar = ROOT.TH2F(\"h_Corr_ttbar\",\"\", len(my_branches_training), 0, len(my_branches_training), len(my_branches_training), 0, len(my_branches_training))\n",
    "for var1 in range(len(my_branches_training)):\n",
    "  h_Corr_Hhh3.GetXaxis().SetBinLabel(var1+1,my_branches_training[var1])\n",
    "  h_Corr_ttbar.GetXaxis().SetBinLabel(var1+1,my_branches_training[var1])\n",
    "  for var2 in range(len(my_branches_training)):\n",
    "    h_Corr_Hhh3.GetYaxis().SetBinLabel(var2+1,my_branches_training[var2])\n",
    "    h_Corr_ttbar.GetYaxis().SetBinLabel(var2+1,my_branches_training[var2])\n",
    "    if(var2>=var1):\n",
    "      array_Var1_Hhh3_var1 = np.array( Hhh3[my_branches_training[var1]] )\n",
    "      array_Var1_Hhh3_var2 = np.array( Hhh3[my_branches_training[var2]] )\n",
    "      array_Var1_ttbar_var1 = np.array( ttbar[my_branches_training[var1]] )\n",
    "      array_Var1_ttbar_var2 = np.array( ttbar[my_branches_training[var2]] )\n",
    "      corr = scipy.stats.pearsonr( array_Var1_Hhh3_var1, array_Var1_Hhh3_var2 )[0]\n",
    "      h_Corr_Hhh3.SetBinContent(var1+1,var2+1,corr)\n",
    "      corr = scipy.stats.pearsonr( array_Var1_ttbar_var1, array_Var1_ttbar_var2 )[0]\n",
    "      h_Corr_ttbar.SetBinContent(var1+1,var2+1,corr)\n",
    "h_Corr_Hhh3.GetZaxis().SetRangeUser(-1.,1.)\n",
    "h_Corr_ttbar.GetZaxis().SetRangeUser(-1.,1.)\n",
    "h_Corr_Hhh3.Draw(\"colzTEXT\")\n",
    "c1.SaveAs(folder + '/Corr_Hhh3.png')\n",
    "h_Corr_ttbar.Draw(\"colzTEXT\")\n",
    "c1.SaveAs(folder + '/Corr_ttbar.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You only need a Dataframe for the training. So you merge all the one you have\n",
    "df =  pd.concat((Hhh3[my_branches_training], ttbar[my_branches_training]), ignore_index=True)\n",
    "# Turn the df the desired ndarray \"X\" that can be directly used for ML applications.\n",
    "X = df.as_matrix() # Each row is an object to classify, each column corresponds to a specific variable.\n",
    "# Take the weights\n",
    "w =  pd.concat((Hhh3['fin_weight'], ttbar['fin_weight']), ignore_index=True).values\n",
    "# This is the array with the true values: 0 is signal, 1 if TT.\n",
    "y = []\n",
    "for _df, ID in [(Hhh3, 0), (ttbar, 1)]:\n",
    "    y.extend([ID] * _df.shape[0])\n",
    "y = np.array(y)\n",
    "print \"You just created X, w, and Y.\"\n",
    "print \" 1) X : matrix with raw=#Events and column=Variables to discriminate.\"\n",
    "print \" 2) w : A vector containig the weights of each event\"\n",
    "print \" 3) X : A vector containing for each event if it is signal (0) or TT (1)\"\n",
    "\n",
    "# Randomly shuffle and automatically split all your objects into train and test subsets\n",
    "ix = range(X.shape[0]) # array of indices, just to keep track of them for safety reasons and future checks\n",
    "X_train, X_test, y_train, y_test, w_train, w_test, ix_train, ix_test = train_test_split(X, y, w, ix, train_size=0.7) # Train here is 70% of the total statistic\n",
    "# It is common practice to scale the inputs to Neural Nets such that they have approximately similar ranges.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron (MLP) definition\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=X_train.shape[1], activation='relu')) # Linear transformation of the input vector. The first number is output_dim.\n",
    "model.add(Dropout(0.1)) # To avoid overfitting. It masks the outputs of the previous layer such that some of them will randomly become inactive and will not contribute to information propagation.\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20, activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Highway(activation='softmax')) # Use adaptive gating units which learn to regulate the flow of information through a network. Improves ability to train very deep feed-forward nets.\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(MaxoutDense(10,10)) # A Dense layer that learns its own activation function. Not working in Tensorflow.\n",
    "model.add(Dense(2, activation='softmax')) # Last layer has to have the same dimensionality as the number of classes we want to predict, here 2.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you need to declare what loss function and optimizer to use (and compile your model).\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "print('---------------------------Training:---------------------------')\n",
    "try:\n",
    "    model.fit(X_train, y_train, validation_split=0.2, nb_epoch=10,\n",
    "              class_weight={\n",
    "                0 : compute_class_weight(\"balanced\", [0, 1], y)[0], # Function that return \"[1/N_classes * ((float(len(y)) / (y == 0).sum())), 1/N_classes * ((float(len(y)) / (y == 1).sum()))]\"\n",
    "                1 : compute_class_weight(\"balanced\", [0, 1], y)[1]\n",
    "              },\n",
    "        callbacks = [\n",
    "            EarlyStopping(verbose=True, patience=6, monitor='val_loss'),\n",
    "            ModelCheckpoint('./models/tutorial-progress.h5', monitor='val_loss', verbose=True, save_best_only=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print 'Training ended early.'\n",
    "# Load the best network (by default you return the last one, you if you save every time you have a better one you are fine loading it later)\n",
    "#model.load_weights('./models/tutorial-progress.h5')\n",
    "print 'Saving weights...'\n",
    "#model.save_weights('./models/tutorial.h5', overwrite=True)\n",
    "#json_string = model.to_json()\n",
    "#open('./models/tutorial.json', 'w').write(json_string)\n",
    "##print 'Testing...'\n",
    "##yhat = model.predict(X_test, verbose = True, batch_size = 512) # yhat is the categorization given by the DNN\n",
    "###Turn them into classes\n",
    "##yhat_cls = np.argmax(yhat, axis=1) # Returns the indices of the maximum values along axis 1\n",
    "##bins = np.linspace(-0.5,1.5,3)\n",
    "##names = ['','Hhh B3','','tt']\n",
    "##fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "##ax = plt.subplot()\n",
    "##ax.set_xticklabels(names, rotation=45)\n",
    "##_ = plt.hist(yhat_cls, bins=bins, histtype='stepfilled', alpha=0.5, label='prediction',log=True)#, weights=w_test)#,log=True)\n",
    "##_ = plt.hist(y_test, bins=bins, histtype='stepfilled', alpha=0.5, label='truth',log=True)#, weights=w_test)#,log=True)\n",
    "##plt.legend(loc='upper right')\n",
    "##print('Saving:',folder + '/Performance.png')\n",
    "##plt.savefig(folder + '/Performance.png')\n",
    "##\n",
    "##print 'Signal efficiency:',     w_test[(y_test == 0) & (yhat_cls == 0)].sum() / w_test[y_test == 0].sum()\n",
    "##print 'Background efficiency:', w_test[(y_test != 0) & (yhat_cls == 0)].sum() / w_test[y_test != 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
