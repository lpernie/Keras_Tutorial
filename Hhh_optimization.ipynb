{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We begin the tutorial importing several classes we will need.\n"
     ]
    }
   ],
   "source": [
    "import sys;\n",
    "print 'We begin the tutorial importing several classes we will need.'\n",
    "import os, sys, array, re, math, random, subprocess, glob\n",
    "from math import *\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy.lib.recfunctions import stack_arrays\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the IMPORT of sklearn, keras, and deepdish.\n"
     ]
    }
   ],
   "source": [
    "print 'Testing the IMPORT of sklearn, keras, and deepdish.'\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Highway, MaxoutDense\n",
    "from keras.layers import Masking, GRU, Merge\n",
    "from keras.layers import Input, merge\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import deepdish.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the IMPORT of pyROOT.\n",
      "I you had no error so far, this is great! We can start the tutorial.\n"
     ]
    }
   ],
   "source": [
    "print 'Testing the IMPORT of pyROOT.'\n",
    "import ROOT\n",
    "from ROOT import gSystem, gROOT, gApplication, TFile, TTree, TCut, TH1F, TCanvas\n",
    "from root_numpy import root2array, root2array \n",
    "print \"I you had no error so far, this is great! We can start the tutorial.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check this file out, it contains many functions we will use.\n",
    "execfile(\"Useful_func.py\")\n",
    "# Fix random seed for reproducibility\n",
    "seed = 7; np.random.seed(seed);\n",
    "# Input paramters\n",
    "debug = True #(Verbose output)\n",
    "folder='Plots_Hhh_tt_MLP/' # Folder with Plots\n",
    "MakePlots=True # Set False if you want to run faster\n",
    "folderCreation  = subprocess.Popen(['mkdir -p ' + folder], stdout=subprocess.PIPE, shell=True); folderCreation.communicate()\n",
    "folderCreation2 = subprocess.Popen(['mkdir -p models/'], stdout=subprocess.PIPE, shell=True); folderCreation2.communicate()\n",
    "\n",
    "# Pre-selection and branches to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our goal is to separate BX Signal from TT background. The machine learning only want to analyze \"good events\"\n",
    "# We defoe god events by: \n",
    "# 1) Applying a preselection to out Signal and background events\n",
    "my_selec = 'MMC_h2massweight1_prob>200 && hasRECOjet1 && hasRECOjet1 && hasMET && hastwomuons && (((b1jet_btag&2)>0 && (b2jet_btag&3)>0) || ((b1jet_btag&3)>0 && (b 2jet_btag&2)>0)) && dR_l1l2<3.3 && dR_l1l2>0.07 && dR_b1b2<5. && mass_l1l2<100 && mass_l1l2>5. && mass_b1b2>150 && dR_bl<5 && dR_l1l2b1b2<6 && MINdR_bl<3.2 && MINdR_bl>0.4 && mass_b1b2<700 && mass_trans<250 && MT2<400 && pt_b1b2<300'\n",
    "# 2) Selecting the branches that contains the information we want to use (in general)\n",
    "my_branches =  [\"dR_l1l2\",\"dR_b1b2\",\"dR_bl\",\"dR_l1l2b1b2\",\"MINdR_bl\",\"dphi_l1l2b1b2\",\"mass_l1l2\",\"mass_b1b2\",\"mass_trans\",\"MT2\",\"pt_b1b2\",\"weight\",\"reweighting\",\"MMC_h2massweight1_prob\"]\n",
    "# 3) Selecting the branches that contains the information we want to use (in the training)\n",
    "my_branches_training = [\"dR_l1l2\",\"dR_b1b2\",\"dR_bl\",\"dR_l1l2b1b2\",\"MINdR_bl\",\"dphi_l1l2b1b2\",\"mass_l1l2\",\"mass_b1b2\",\"mass_trans\",\"MT2\",\"pt_b1b2\",\"MMC_h2massweight1_prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Hhh3 Displayed as panda dataframe: \n",
      "      dR_l1l2   dR_b1b2     dR_bl  dR_l1l2b1b2  MINdR_bl  dphi_l1l2b1b2  \\\n",
      "0    0.835309  2.426065  1.771351     2.456219  1.771351       2.439342   \n",
      "1    1.680773  3.461864  2.625820     3.189628  1.296153       3.104695   \n",
      "2    2.613559  3.463338  1.169699     1.112213  1.169699       0.488734   \n",
      "3    0.555706  2.797136  2.208439     3.274508  1.852719       1.813255   \n",
      "4    1.265501  3.291811  2.542928     2.743070  2.019794       1.932036   \n",
      "5    0.789705  2.080813  2.188340     2.450307  1.663508       2.449623   \n",
      "6    0.134017  3.206850  3.176847     3.489993  1.842429       2.511945   \n",
      "7    0.999802  2.457096  2.990820     3.072848  2.166252       1.740511   \n",
      "8    1.278717  3.796422  3.203525     3.048047  0.625945       1.071269   \n",
      "9    0.753285  3.444453  3.334925     3.395689  1.011419       2.675913   \n",
      "10   2.449032  2.889399  1.960407     3.281974  0.530522       3.125057   \n",
      "11   0.874256  2.122234  1.336319     1.749813  1.312641       1.497477   \n",
      "12   1.252664  4.528258  3.254978     2.716770  1.927995       2.483346   \n",
      "13   1.852161  2.620967  3.202087     2.517139  0.793235       1.570765   \n",
      "14   2.594584  2.724143  1.181497     2.110318  0.601314       1.632751   \n",
      "15   0.763292  2.893050  2.385521     2.389899  1.680420       2.308819   \n",
      "16   1.200446  3.000677  1.721923     2.017189  0.784079       2.006371   \n",
      "17   1.091338  3.504239  3.369758     3.288020  0.986141       2.858679   \n",
      "18   1.266449  3.089469  2.703415     3.831539  1.339258       2.788022   \n",
      "19   0.668421  2.846026  2.667840     3.191338  1.229645       3.080104   \n",
      "20   0.777605  2.963259  1.513624     0.743358  1.178037       0.704749   \n",
      "21   0.721134  3.198812  0.991122     1.660986  0.991122       1.024605   \n",
      "22   2.562762  3.626119  2.906000     4.566886  1.171780       2.638563   \n",
      "23   0.896529  3.283195  3.112850     3.523150  2.890329       2.461356   \n",
      "24   0.859413  2.571202  1.845904     1.952992  1.628875       1.889527   \n",
      "25   0.981043  3.723560  4.378975     4.749786  1.501294       3.071962   \n",
      "26   0.877206  3.101734  2.199878     4.114049  1.878264       2.757962   \n",
      "27   2.399165  2.461232  2.847996     2.564544  0.863253       2.472659   \n",
      "28   2.892444  3.498046  1.434095     4.669363  1.434095       0.283562   \n",
      "29   1.136582  2.144633  3.226872     3.132311  2.031791       2.215731   \n",
      "..        ...       ...       ...          ...       ...            ...   \n",
      "500  2.529044  2.414778  2.939116     3.921067  1.629285       2.607891   \n",
      "501  0.652963  2.965158  2.577468     3.354337  1.121260       2.898621   \n",
      "502  1.346197  3.181867  2.600639     2.982042  0.987796       2.711081   \n",
      "503  0.420347  2.083685  2.373605     2.970505  1.744384       2.949119   \n",
      "504  1.543845  2.332436  3.435144     3.046634  1.024859       2.627467   \n",
      "505  2.275064  3.257009  3.009696     2.706322  1.328013       2.696669   \n",
      "506  1.393184  3.516658  1.413067     3.491076  1.413067       1.433716   \n",
      "507  2.598977  3.233184  3.462076     3.237266  1.676834       2.977386   \n",
      "508  0.338443  2.140446  2.415453     2.602841  2.241373       2.581390   \n",
      "509  0.510070  2.961539  2.327730     1.996403  1.728824       1.975286   \n",
      "510  2.886883  2.167383  2.098886     2.648281  1.515010       1.438921   \n",
      "511  1.086884  3.753212  4.855115     5.374489  0.749636       3.111125   \n",
      "512  2.277160  2.825130  2.495667     3.456007  1.327736       0.097730   \n",
      "513  0.330407  1.946128  1.993561     2.703932  1.993561       2.701999   \n",
      "514  0.734674  3.585384  3.180341     3.524325  1.765797       2.934818   \n",
      "515  0.985028  2.345827  1.243207     2.375080  1.243207       2.372957   \n",
      "516  0.894168  2.037879  2.912406     3.374961  2.111708       3.120372   \n",
      "517  0.978760  3.492046  4.148928     3.083558  0.886251       2.570038   \n",
      "518  0.822257  2.129521  3.138035     2.187747  1.087091       2.187721   \n",
      "519  0.505481  2.830483  2.076861     3.115632  1.834924       3.114201   \n",
      "520  1.433986  2.701696  3.532020     4.728498  2.300848       2.836781   \n",
      "521  0.883792  2.133748  1.576008     1.713516  1.017971       1.673995   \n",
      "522  1.391397  3.396182  1.175586     3.829099  1.175586       1.122743   \n",
      "523  1.492445  3.871244  3.833133     4.192673  1.048026       2.464498   \n",
      "524  0.515271  3.537918  2.369087     3.521550  1.636223       2.334546   \n",
      "525  0.734172  3.559367  3.626001     3.803191  2.276654       2.853725   \n",
      "526  0.360762  3.309613  2.759387     3.086255  1.216934       2.898400   \n",
      "527  0.242987  2.975497  2.931324     3.080765  1.393061       3.017611   \n",
      "528  0.879263  2.436267  3.045939     3.378758  1.597500       2.815749   \n",
      "529  0.522640  2.583189  3.675986     3.815724  2.080396       2.880811   \n",
      "\n",
      "     mass_l1l2   mass_b1b2  mass_trans         MT2     pt_b1b2    weight  \\\n",
      "0    26.154713  154.027679   58.967606  108.648064   75.866676  0.036115   \n",
      "1    48.760616  289.019287   28.863323  149.505524   28.862673  0.036115   \n",
      "2    43.728931  173.601822   54.721973  122.493980   79.129578  0.036115   \n",
      "3    10.754784  216.893616   36.073021  153.440765  106.172356  0.036115   \n",
      "4    50.720337  288.784607   80.369232  137.754059   79.815788  0.036115   \n",
      "5    22.443901  230.591522   40.370525  189.168457  198.284348  0.036115   \n",
      "6    10.010504  294.528503  184.254715  313.727051  115.754448  0.036115   \n",
      "7    22.730286  270.327576   63.193134  172.777634  125.832314  0.036115   \n",
      "8    46.244183  154.165817  134.452560   82.540794   13.630374  0.036115   \n",
      "9    33.799641  179.536209  101.177376  148.808136   67.228294  0.036115   \n",
      "10   38.328079  237.172821   10.291577  103.337639   91.853676  0.036115   \n",
      "11   31.876579  157.516861   79.239006  172.407959  141.131638  0.036115   \n",
      "12   39.392387  350.154205   59.718025   99.025642   45.637569  0.036115   \n",
      "13   58.513153  167.642349  108.368271  124.144096   47.111309  0.036115   \n",
      "14   52.255062  183.840195  115.602600   87.859642   55.705593  0.036115   \n",
      "15   28.087557  166.378723  101.264053  104.962357   94.578728  0.036115   \n",
      "16   48.293861  230.286453   94.091454  159.181503  128.808655  0.036115   \n",
      "17   34.790268  234.041840  150.897690  128.731033   85.472221  0.036115   \n",
      "18   49.638599  547.271973   14.906613  309.944153  185.213623  0.036115   \n",
      "19   40.609921  172.786224   80.553398  206.028030  141.480164  0.036115   \n",
      "20   34.669735  161.972931   47.623173  120.763130   70.600212  0.036115   \n",
      "21   23.300365  180.363983   65.249466   90.227341   81.708649  0.036115   \n",
      "22   52.124851  438.103638   33.047466  183.055603   63.262943  0.036115   \n",
      "23   26.695665  182.083313   80.820328  129.027893   56.821625  0.036115   \n",
      "24   41.349129  283.409912  107.731102  162.485672  155.540695  0.036115   \n",
      "25   34.083630  196.770935    9.639222  163.594559   45.346043  0.036115   \n",
      "26   46.580109  305.611572  158.656128  295.103302  104.589149  0.036115   \n",
      "27   31.979269  262.349091   19.444038  117.424934  109.837685  0.036115   \n",
      "28   42.151318  215.668701   65.834389  196.790405   34.564465  0.036115   \n",
      "29   44.472313  194.009201  121.698715  231.228775  106.649315  0.036115   \n",
      "..         ...         ...         ...         ...         ...       ...   \n",
      "500  74.574432  151.359802   31.963034  139.626587   62.083115  0.036115   \n",
      "501  18.376339  252.707321   78.457092  114.529854   73.472282  0.036115   \n",
      "502  72.266563  331.630890   25.348894  205.796280  175.527649  0.036115   \n",
      "503  13.512495  160.685165  132.571548  121.233437  102.558945  0.036115   \n",
      "504  61.448372  223.059341   75.000519  170.533859  128.272247  0.036115   \n",
      "505  76.152069  334.795868   58.674557  177.612289  142.794434  0.036115   \n",
      "506  60.350082  224.490524  128.685989  136.715118   15.892715  0.036115   \n",
      "507  58.434017  304.529541   13.818085  167.251343  139.495834  0.036115   \n",
      "508  12.464584  161.671356   90.183655  167.696579  125.149055  0.036115   \n",
      "509  15.086629  222.098206  111.464890  150.449203  117.923790  0.036115   \n",
      "510  50.258102  182.130081   82.641640  164.219116  114.894798  0.036115   \n",
      "511  47.070362  230.865326    5.978599  233.274857   27.312775  0.036115   \n",
      "512  39.708809  169.111710   17.848652  180.020096  113.992241  0.036115   \n",
      "513  19.156149  156.258865   60.890533  164.600006  113.347198  0.036115   \n",
      "514  44.623383  630.214905  108.262047  278.097992  198.715134  0.036115   \n",
      "515  30.182928  312.069489   63.961021  169.382080  148.472992  0.036115   \n",
      "516  40.853401  217.501160   33.711018  203.213745  148.864685  0.036115   \n",
      "517  41.757847  220.175797  120.805084  192.352905   57.499241  0.036115   \n",
      "518  35.384346  194.598145   89.183769  151.668945  145.970490  0.036115   \n",
      "519  16.154282  182.049820   44.787766  150.611343   46.866463  0.036115   \n",
      "520  38.241470  177.351349   64.995979  161.088898   43.949276  0.036115   \n",
      "521  39.373127  154.459549  177.994492  139.792755   90.965141  0.036115   \n",
      "522  63.862350  522.720154   88.508377  187.564011   13.326844  0.036115   \n",
      "523  54.356922  633.582397   89.160187  253.769821  182.762222  0.036115   \n",
      "524  13.241815  607.181824   78.049271  142.928085   64.540443  0.036115   \n",
      "525  31.968111  212.942505   97.207153  141.148605   56.773438  0.036115   \n",
      "526  14.006442  211.701050  155.427002  167.308273  100.496147  0.036115   \n",
      "527  13.277418  156.938980  137.196869  144.359314   87.922722  0.036115   \n",
      "528  41.795815  255.330948  105.241051  171.189011  168.547028  0.036115   \n",
      "529  36.388149  242.946960  124.943298  342.396759  195.963043  0.036115   \n",
      "\n",
      "     reweighting  MMC_h2massweight1_prob  fin_weight  \n",
      "0       1.403897                   313.5    0.050702  \n",
      "1       1.443911                   267.5    0.052147  \n",
      "2       1.550185                   307.5    0.055985  \n",
      "3       1.695638                   343.5    0.061238  \n",
      "4       1.307515                   354.5    0.047221  \n",
      "5       1.491994                   442.5    0.053883  \n",
      "6       1.263788                   398.5    0.045642  \n",
      "7       1.612605                   371.5    0.058239  \n",
      "8       1.401984                   294.5    0.050633  \n",
      "9       1.416387                   443.5    0.051153  \n",
      "10      1.527794                   339.5    0.055176  \n",
      "11      1.389088                   385.5    0.050167  \n",
      "12      1.686601                   309.5    0.060911  \n",
      "13      1.527584                   280.5    0.055169  \n",
      "14      1.462631                   277.5    0.052823  \n",
      "15      1.413434                   320.5    0.051046  \n",
      "16      1.261144                   352.5    0.045546  \n",
      "17      1.406998                   374.5    0.050814  \n",
      "18      1.504693                   468.5    0.054342  \n",
      "19      1.215964                   431.5    0.043914  \n",
      "20      1.304798                   290.5    0.047123  \n",
      "21      1.506359                   335.5    0.054402  \n",
      "22      1.390097                   377.5    0.050203  \n",
      "23      1.457264                   292.5    0.052629  \n",
      "24      1.329476                   372.5    0.048014  \n",
      "25      1.414166                   537.5    0.051072  \n",
      "26      1.254968                   395.5    0.045323  \n",
      "27      1.663386                   378.5    0.060073  \n",
      "28      1.000000                   384.5    0.036115  \n",
      "29      1.496456                   441.5    0.054044  \n",
      "..           ...                     ...         ...  \n",
      "500     1.426978                   343.5    0.051535  \n",
      "501     1.525462                   361.5    0.055092  \n",
      "502     1.317994                   519.5    0.047599  \n",
      "503     1.371945                   286.5    0.049548  \n",
      "504     1.338681                   462.5    0.048346  \n",
      "505     1.060296                   491.5    0.038292  \n",
      "506     1.422405                   330.5    0.051370  \n",
      "507     1.391988                   456.5    0.050272  \n",
      "508     1.399732                   345.5    0.050551  \n",
      "509     1.497452                   369.5    0.054080  \n",
      "510     1.582384                   328.5    0.057148  \n",
      "511     1.443578                   631.5    0.052135  \n",
      "512     1.568670                   373.5    0.056652  \n",
      "513     1.338597                   352.5    0.048343  \n",
      "514     1.321856                   521.5    0.047739  \n",
      "515     1.235141                   279.5    0.044607  \n",
      "516     1.423416                   456.5    0.051407  \n",
      "517     1.359379                   551.5    0.049094  \n",
      "518     1.341353                   380.5    0.048443  \n",
      "519     1.494131                   370.5    0.053960  \n",
      "520     1.458318                   388.5    0.052667  \n",
      "521     1.355191                   350.5    0.048943  \n",
      "522     1.387648                   270.5    0.050115  \n",
      "523     1.397243                   833.5    0.050461  \n",
      "524     1.497452                   257.5    0.054080  \n",
      "525     1.258291                   455.5    0.045443  \n",
      "526     1.459283                   351.5    0.052702  \n",
      "527     1.282725                   339.5    0.046325  \n",
      "528     1.357510                   559.5    0.049026  \n",
      "529     1.248533                   886.5    0.045091  \n",
      "\n",
      "[530 rows x 15 columns]\n",
      "The shape for Hhh3 is [nb_events, nb_variables]: \n",
      "(530, 15)\n",
      "The shape for tt is [nb_events, nb_variables]: \n",
      "(1902, 15)\n"
     ]
    }
   ],
   "source": [
    "# Converting Root files in dataframe (Very useful, checnl root2panda in Useful_func.py)\n",
    "Hhh3  = root2panda('files_root/delphes_B3_1M_PU40ALL_1Ag_mvammc_TRIMMED.root', 'evtree', branches=my_branches, selection=my_selec)\n",
    "ttbar = root2panda('files_root/delphes_tt_4M_PU40_WtomuALL_1Ag_mvammc_TRIMMED_5Kevents.root', 'evtree', branches=my_branches, selection=my_selec)\n",
    "# Create a variable that is the total weight (weight=weight for xsec, reweighting=weight depending on muons)\n",
    "Hhh3['fin_weight']  = Hhh3['weight']*Hhh3['reweighting']\n",
    "ttbar['fin_weight'] = ttbar['weight']*ttbar['reweighting']\n",
    "## Save the dataframe as h5 file (for quick loading in the future).\n",
    "#io.save(open('models/ttbar.h5', 'wb'), ttbar)\n",
    "#ttbar = io.load(open('models/ttbar.h5', 'rb'))\n",
    "\n",
    "if debug:\n",
    "  print(\"---> Hhh3 Displayed as panda dataframe: \"); print(Hhh3)\n",
    "  print(\"The shape for Hhh3 is [nb_events, nb_variables]: \"); print(Hhh3.shape)\n",
    "  print(\"The shape for tt is [nb_events, nb_variables]: \"); print(ttbar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Saving:', 'Plots_Hhh_tt_MLP//dR_l1l2.png')\n",
      "('Saving:', 'Plots_Hhh_tt_MLP//dR_b1b2.png')\n",
      "('Saving:', 'Plots_Hhh_tt_MLP//dR_bl.png')\n",
      "('Saving:', 'Plots_Hhh_tt_MLP//dR_l1l2b1b2.png')\n",
      "('Saving:', 'Plots_Hhh_tt_MLP//MINdR_bl.png')\n",
      "('Saving:', 'Plots_Hhh_tt_MLP//dphi_l1l2b1b2.png')\n",
      "('Saving:', 'Plots_Hhh_tt_MLP//mass_l1l2.png')\n",
      "('Saving:', 'Plots_Hhh_tt_MLP//mass_b1b2.png')\n",
      "('Saving:', 'Plots_Hhh_tt_MLP//mass_trans.png')\n",
      "('Saving:', 'Plots_Hhh_tt_MLP//MT2.png')\n",
      "('Saving:', 'Plots_Hhh_tt_MLP//pt_b1b2.png')\n",
      "('Saving:', 'Plots_Hhh_tt_MLP//MMC_h2massweight1_prob.png')\n"
     ]
    }
   ],
   "source": [
    "# Plots of the branches we selected\n",
    "if (MakePlots) :\n",
    "  for key in ttbar.keys() :\n",
    "      if(key!=\"weight\" and key!=\"reweighting\" and key!=\"fin_weight\") :\n",
    "        matplotlib.rcParams.update({'font.size': 16})\n",
    "        fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "        bins = np.linspace(my_max(min(ttbar[key]),0.), max(ttbar[key]), 50)\n",
    "        _ = plt.hist(Hhh3[key],  bins=bins, histtype='step', normed=True, label=r'$Hhh B3$', linewidth=2)\n",
    "        _ = plt.hist(ttbar[key], bins=bins, histtype='step', normed=True, label=r'$t\\overline{t}$')\n",
    "        plt.xlabel(key)\n",
    "        plt.ylabel('Entries')\n",
    "        plt.legend(loc='best')\n",
    "        print('Saving:',folder + '/' + str(key) + '.png')\n",
    "        plt.savefig(folder + \"/\" + str(key) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now lets start to talk about DNN!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TROOT::Append>: Replacing existing TH1: h_Corr_Hhh3 (Potential memory leak).\n",
      "Warning in <TROOT::Append>: Replacing existing TH1: h_Corr_ttbar (Potential memory leak).\n",
      "Info in <TCanvas::Print>: png file Plots_Hhh_tt_MLP//Corr_Hhh3.png has been created\n",
      "Info in <TCanvas::Print>: png file Plots_Hhh_tt_MLP//Corr_ttbar.png has been created\n"
     ]
    }
   ],
   "source": [
    "print('Now lets start to talk about DNN!')\n",
    "# Not lets draw the correlations\n",
    "c1 = ROOT.TCanvas(); c1.cd(); ROOT.gStyle.SetOptStat(0)\n",
    "h_Corr_Hhh3  = ROOT.TH2F(\"h_Corr_Hhh3\",\"\", len(my_branches_training), 0, len(my_branches_training), len(my_branches_training), 0, len(my_branches_training))\n",
    "h_Corr_ttbar = ROOT.TH2F(\"h_Corr_ttbar\",\"\", len(my_branches_training), 0, len(my_branches_training), len(my_branches_training), 0, len(my_branches_training))\n",
    "for var1 in range(len(my_branches_training)):\n",
    "  h_Corr_Hhh3.GetXaxis().SetBinLabel(var1+1,my_branches_training[var1])\n",
    "  h_Corr_ttbar.GetXaxis().SetBinLabel(var1+1,my_branches_training[var1])\n",
    "  for var2 in range(len(my_branches_training)):\n",
    "    h_Corr_Hhh3.GetYaxis().SetBinLabel(var2+1,my_branches_training[var2])\n",
    "    h_Corr_ttbar.GetYaxis().SetBinLabel(var2+1,my_branches_training[var2])\n",
    "    if(var2>=var1):\n",
    "      array_Var1_Hhh3_var1 = np.array( Hhh3[my_branches_training[var1]] )\n",
    "      array_Var1_Hhh3_var2 = np.array( Hhh3[my_branches_training[var2]] )\n",
    "      array_Var1_ttbar_var1 = np.array( ttbar[my_branches_training[var1]] )\n",
    "      array_Var1_ttbar_var2 = np.array( ttbar[my_branches_training[var2]] )\n",
    "      corr = scipy.stats.pearsonr( array_Var1_Hhh3_var1, array_Var1_Hhh3_var2 )[0]\n",
    "      h_Corr_Hhh3.SetBinContent(var1+1,var2+1,corr)\n",
    "      corr = scipy.stats.pearsonr( array_Var1_ttbar_var1, array_Var1_ttbar_var2 )[0]\n",
    "      h_Corr_ttbar.SetBinContent(var1+1,var2+1,corr)\n",
    "h_Corr_Hhh3.GetZaxis().SetRangeUser(-1.,1.)\n",
    "h_Corr_ttbar.GetZaxis().SetRangeUser(-1.,1.)\n",
    "h_Corr_Hhh3.Draw(\"colzTEXT\")\n",
    "c1.SaveAs(folder + '/Corr_Hhh3.png')\n",
    "h_Corr_ttbar.Draw(\"colzTEXT\")\n",
    "c1.SaveAs(folder + '/Corr_ttbar.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You just created X, w, and Y.\n",
      " 1) X : matrix with raw=#Events and column=Variables to discriminate.\n",
      " 2) w : A vector containig the weights of each event\n",
      " 3) X : A vector containing for each event if it is signal (0) or TT (1)\n"
     ]
    }
   ],
   "source": [
    "#You only need a Dataframe for the training. So you merge all the one you have\n",
    "df =  pd.concat((Hhh3[my_branches_training], ttbar[my_branches_training]), ignore_index=True)\n",
    "# Turn the df the desired ndarray \"X\" that can be directly used for ML applications.\n",
    "X = df.as_matrix() # Each row is an object to classify, each column corresponds to a specific variable.\n",
    "# Take the weights\n",
    "w =  pd.concat((Hhh3['fin_weight'], ttbar['fin_weight']), ignore_index=True).values\n",
    "# This is the array with the true values: 0 is signal, 1 if TT.\n",
    "y = []\n",
    "for _df, ID in [(Hhh3, 0), (ttbar, 1)]:\n",
    "    y.extend([ID] * _df.shape[0])\n",
    "y = np.array(y)\n",
    "print \"You just created X, w, and Y.\"\n",
    "print \" 1) X : matrix with raw=#Events and column=Variables to discriminate.\"\n",
    "print \" 2) w : A vector containig the weights of each event\"\n",
    "print \" 3) X : A vector containing for each event if it is signal (0) or TT (1)\"\n",
    "\n",
    "# Randomly shuffle and automatically split all your objects into train and test subsets\n",
    "ix = range(X.shape[0]) # array of indices, just to keep track of them for safety reasons and future checks\n",
    "X_train, X_test, y_train, y_test, w_train, w_test, ix_train, ix_test = train_test_split(X, y, w, ix, train_size=0.7) # Train here is 70% of the total statistic\n",
    "# It is common practice to scale the inputs to Neural Nets such that they have approximately similar ranges.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/python/bin/python2.7\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
