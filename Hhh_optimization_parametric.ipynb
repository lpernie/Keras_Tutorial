{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS and TENSORFLOW\n",
    "TUTORIAL: Using a Multi Layer Perceptron in order to separate hh from TTbar. In example we train several masses h2 masses instead than a single mass point. \n",
    "(%lsmagic for magic commanda and ! for bash instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I you had no error so far, this is great! We can start the tutorial.\n"
     ]
    }
   ],
   "source": [
    "# We start by importing all the necessary packages\n",
    "import sys;\n",
    "import os, sys, array, re, math, random, subprocess, glob\n",
    "from math import *\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy.lib.recfunctions import stack_arrays\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Highway, MaxoutDense, Masking, GRU, Merge, Input, merge\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import deepdish.io as io\n",
    "import ROOT\n",
    "from ROOT import gSystem, gROOT, gApplication, TFile, TTree, TCut, TH1F, TCanvas\n",
    "from root_numpy import root2array \n",
    "from IPython.display import HTML, IFrame\n",
    "import seaborn as sns; sns.set()\n",
    "print \"I you had no error so far, this is great! We can start the tutorial.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set some variables we will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check this file out, it contains many functions we will use.\n",
    "execfile(\"Useful_func.py\")\n",
    "# Fix random seed for reproducibility\n",
    "seed = 7; np.random.seed(seed);\n",
    "# Input paramters\n",
    "debug = True #(Verbose output)\n",
    "folder='Plots_hh_tt_parametric_MLP' # Folder with Plots\n",
    "MakePlots=True # Set False if you want to run faster\n",
    "folderCreation  = subprocess.Popen(['mkdir -p ' + folder], stdout=subprocess.PIPE, shell=True); folderCreation.communicate()\n",
    "folderCreation2 = subprocess.Popen(['mkdir -p models/'], stdout=subprocess.PIPE, shell=True); folderCreation2.communicate()\n",
    "h_file1 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-260_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file2 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-270_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file3 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-300_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file4 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-350_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file5 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-400_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file6 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-450_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file7 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-500_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file8 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-550_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file9 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-600_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file10 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-650_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file11 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-750_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file12 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-800_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file13 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-900_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file14 = \"files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-1000_narrow_13TeV-madgraph-v2.root\"\n",
    "h_file_ALL = [h_file1,h_file2,h_file3,h_file4,h_file5,h_file6,h_file7,h_file8,h_file9,h_file10,h_file11,h_file12,h_file13,h_file14]\n",
    "TT_df_list = ['files/tt_dataframe_0.csv','files/tt_dataframe_1.csv','files/tt_dataframe_2.csv','files/tt_dataframe_3.csv','files/tt_dataframe_4.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now We Start manipulating ROOT files into a format we can use to train a MVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our goal is to separate BX Signal from TT background. The machine learning only want to analyze \"good events\": \n",
    "# 1) Applying a preselection to out Signal and background events\n",
    "my_selec = 'met_pt>20 && met_pt<500 && muon1_pt>20 && fabs(muon1_eta)<2.4 && muon2_pt>10 && fabs(muon2_eta)<2.4 && pt_l1l2<500 && pt_b1b2<500 && mass_l1l2>12 && mass_l1l2<500 && mass_b1b2<500 && b1jet_pt>20 && fabs(b1jet_eta)<2.4 && b2jet_pt>20 && fabs(b2jet_eta)<2.4 && mass_trans>10 && mass_trans<500 && HT<4000'\n",
    "# 2) Selecting the branches that contains the information we want to use (in general)\n",
    "my_branches = [\"mass_trans\",\"dphi_llmet\",\"dphi_llbb\",\"eta_l1l2\",\"pt_l1l2\",\"mass_l1l2\",\"eta_b1b2\",\"pt_b1b2\",\"mass_b1b2\",\"dR_minbl\",\"dR_l1l2b1b2\",\"HT\",\"met_pt\",\"muon1_pogSF\",\"muon2_pogSF\",\"XsecBr\"]\n",
    "# 3) Selecting the branches that contains the information we want to use (in the training)\n",
    "my_branches_training = [\"mass_trans\",\"dphi_llmet\",\"dphi_llbb\",\"eta_l1l2\",\"pt_l1l2\",\"mass_l1l2\",\"eta_b1b2\",\"pt_b1b2\",\"mass_b1b2\",\"dR_minbl\",\"dR_l1l2b1b2\",\"HT\",\"met_pt\",\"mass\"]\n",
    "    \n",
    "# Converting Root files in dataframe (Very useful, checnl root2panda in Useful_func.py)\n",
    "hh1     = root2panda(h_file1, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh2     = root2panda(h_file2, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh3     = root2panda(h_file3, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh4     = root2panda(h_file4, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh5     = root2panda(h_file5, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh6     = root2panda(h_file6, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh7     = root2panda(h_file7, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh8     = root2panda(h_file8, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh9     = root2panda(h_file9, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh10    = root2panda(h_file10, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh11    = root2panda(h_file11, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh12    = root2panda(h_file12, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh13    = root2panda(h_file13, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "hh14    = root2panda(h_file14, 'DiHiggsWWBBAna/evtree', branches=my_branches, selection=my_selec)\n",
    "HH_ALL = [hh1,hh2,hh3,hh4,hh5,hh6,hh7,hh8,hh9,hh10,hh11,hh12,hh13,hh14]\n",
    "sampleName = ['Radion_260','Radion_270','Radion_300','Radion_350','Radion_400','Radion_450','Radion_500','Radion_550','Radion_600','Radion_650','Radion_750','Radion_800','Radion_900','Radion_1000']\n",
    "## TT is so heavy that I already saved the final dataframe (and I splitted it if 5 so that each df is smaller than 100 Mb).\n",
    "ttbar_df_list = (pd.read_csv(f_df) for f_df in TT_df_list)\n",
    "ttbar = pd.concat(ttbar_df_list, ignore_index=True)                    \n",
    "ttbar = ttbar.drop(ttbar.columns[[0, 1]], 1) #Remove 2 extra columns we have in this df, so it match with Signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assuming a Signal ( Radion_260 ) of 1pb (B.R. included) you expect, after the preselection, to have: 404.458 events\n",
      "Assuming a Signal ( Radion_270 ) of 1pb (B.R. included) you expect, after the preselection, to have: 417.438 events\n",
      "Assuming a Signal ( Radion_300 ) of 1pb (B.R. included) you expect, after the preselection, to have: 505.153 events\n",
      "Assuming a Signal ( Radion_350 ) of 1pb (B.R. included) you expect, after the preselection, to have: 624.786 events\n",
      "Assuming a Signal ( Radion_400 ) of 1pb (B.R. included) you expect, after the preselection, to have: 735.564 events\n",
      "Assuming a Signal ( Radion_450 ) of 1pb (B.R. included) you expect, after the preselection, to have: 814.464 events\n",
      "Assuming a Signal ( Radion_500 ) of 1pb (B.R. included) you expect, after the preselection, to have: 889.349 events\n",
      "Assuming a Signal ( Radion_550 ) of 1pb (B.R. included) you expect, after the preselection, to have: 983.503 events\n",
      "Assuming a Signal ( Radion_600 ) of 1pb (B.R. included) you expect, after the preselection, to have: 1024.36 events\n",
      "Assuming a Signal ( Radion_650 ) of 1pb (B.R. included) you expect, after the preselection, to have: 1082.86 events\n",
      "Assuming a Signal ( Radion_750 ) of 1pb (B.R. included) you expect, after the preselection, to have: 1130.5 events\n",
      "Assuming a Signal ( Radion_800 ) of 1pb (B.R. included) you expect, after the preselection, to have: 1141.58 events\n",
      "Assuming a Signal ( Radion_900 ) of 1pb (B.R. included) you expect, after the preselection, to have: 1163.10924398 events\n",
      "Assuming a Signal ( Radion_1000 ) of 1pb (B.R. included) you expect, after the preselection, to have: 1060.28 events\n",
      "Assuming a background of 87pb (B.R. included) you expect, after the preselection, to have: 55716.5898397 events.\n"
     ]
    }
   ],
   "source": [
    "# These processes have a different Xsextion. You need to create a variable that store weight that in our case it the weight to apply to each event so that they match the expected number of events for a given luminosity. Also you can add other weigths (like Data-MC scale factors) if you have them.\n",
    "# First you need to knwow the Total number of MC events generated\n",
    "Lumi = 36.42 * 1000 #1000 is for passing from fb-1 to pb-1 (in which the xsec is expressed).\n",
    "for iH in range(len(h_file_ALL)):\n",
    "    MyFile_hh =  ROOT.TFile.Open(h_file_ALL[iH],\"read\");\n",
    "    h_prehlt_hh = ROOT.TH1F(MyFile_hh.Get(\"TriggerResults/hevent_filter\"))\n",
    "    nTOT_prehlt_hh = h_prehlt_hh.GetBinContent(2) # Here is stored the number of total event generated\n",
    "    HH_ALL[iH]['XsecBr'] = HH_ALL[iH]['XsecBr']*Lumi/nTOT_prehlt_hh\n",
    "    # Add all weights in the df \n",
    "    HH_ALL[iH]['fin_weight']    = HH_ALL[iH]['XsecBr'] * HH_ALL[iH]['muon1_pogSF'] * HH_ALL[iH]['muon2_pogSF'] #1pb is S Xsec.\n",
    "    print \"Assuming a Signal (\",sampleName[iH],\") of 1pb (B.R. included) you expect, after the preselection, to have:\",HH_ALL[iH]['fin_weight'].sum(),\"events\"\n",
    "nTOT_prehlt_ttbar = 102114184 #Harcoded\n",
    "ttbar['XsecBr'] = ttbar['XsecBr']*Lumi/nTOT_prehlt_ttbar\n",
    "ttbar['fin_weight'] = ttbar['XsecBr'] * ttbar['muon1_pogSF'] * ttbar['muon2_pogSF'] #87pb is TT Xsec.\n",
    "print \"Assuming a background of 87pb (B.R. included) you expect, after the preselection, to have:\",ttbar['fin_weight'].sum(),\"events.\"\n",
    "                \n",
    "## Alternatively you can save a df as a h5 file (for quick loading in the future)\n",
    "#  Ex: io.save(open('models/ttbar.h5', 'wb'), ttbar); ttbar = io.load(open('models/ttbar.h5', 'rb'));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now let's add a parameter in each df that represet the mass\n",
      "---> HH_ALL[0] Displayed as panda dataframe: \n",
      "      mass_trans  dphi_llmet  dphi_llbb  eta_l1l2     pt_l1l2  mass_l1l2  \\\n",
      "0      58.836517    2.036996  -2.195011 -1.761931   53.369804  47.703281   \n",
      "1      91.472862    1.837220   1.374906 -1.877273   52.681824  24.148767   \n",
      "2      84.094986    1.812677  -0.262167 -0.938847   66.133118  64.216621   \n",
      "3     113.967056    2.257362  -1.764214 -0.466149   71.513741  26.362066   \n",
      "4      62.980377   -1.595165   0.880271  0.067176   39.395412  34.028343   \n",
      "5      94.639915   -2.998709   2.102583 -0.483183   58.199478  15.722157   \n",
      "6     127.376617    2.766692  -2.421544 -1.769549   61.554604  13.851027   \n",
      "7      12.867005   -0.165128  -0.968602  0.159346   59.969578  57.782337   \n",
      "8     118.501419   -2.763937   0.571166 -0.473403   37.067215  44.826050   \n",
      "9     103.312714   -2.600723  -2.595056 -1.248913   62.974575  43.908936   \n",
      "10     49.520870    1.162006   3.122833 -0.601111   65.717010  37.003166   \n",
      "11     10.860861    0.174303   0.523037 -0.302437   72.682579  32.114651   \n",
      "12     99.726784   -2.275174   2.732499 -1.148901   60.797684  19.554337   \n",
      "13    106.370499    2.700573   2.933781 -0.229070   40.413643  38.369675   \n",
      "14     41.775749    0.912342   0.414880 -1.589521   61.904736  46.995262   \n",
      "15    125.015053    2.947515  -0.157566  0.513011   86.776184  33.154068   \n",
      "16     60.009438   -1.489696   2.632697 -0.991369   40.577206  26.218649   \n",
      "17    129.882126   -2.972530  -1.446870 -1.906814   47.156582  47.669827   \n",
      "18     37.732498   -1.009288  -3.048392 -0.017961   64.195930  30.342325   \n",
      "19    120.177406    2.195523   1.755333  0.289091   55.294392  19.641584   \n",
      "20     62.132427   -2.842272   1.335036  2.083241   47.527420  32.672268   \n",
      "21     57.381893    1.694512   2.787614 -0.863365   27.307816  82.401817   \n",
      "22     79.531738   -3.006287  -0.463124  0.168127   32.388130  37.343712   \n",
      "23    106.330147   -2.698675   0.580189  0.207878   50.178169  41.573112   \n",
      "24    120.715202    2.217733  -1.108785  0.784938   64.228241  49.933327   \n",
      "25    106.328690    2.201438   0.631411  0.757494   82.618797  40.211094   \n",
      "26     40.174305   -0.770703  -0.662759  1.368303   94.039482  40.207001   \n",
      "27     41.452347    1.256565  -1.428797  1.679871   34.364189  32.021549   \n",
      "28    117.122200   -2.456956   1.675840  1.218370   57.451664  24.812353   \n",
      "29     75.618958    2.606680  -1.431514 -2.023010   42.491066  20.029436   \n",
      "...          ...         ...        ...       ...         ...        ...   \n",
      "3465  106.278923   -2.398947   2.163876  0.681877   37.027702  18.177782   \n",
      "3466  112.597260    3.028439  -2.520658  2.221131   60.451965  26.978662   \n",
      "3467  104.401596    2.574034   3.039248 -1.559526   52.170006  54.820343   \n",
      "3468   55.613338    1.077785   0.454832 -1.625174   31.720068  47.601128   \n",
      "3469   76.103264    1.899990  -2.724061  1.635557   53.301800  20.411690   \n",
      "3470   53.015251    2.259204   2.214338  0.213479   25.587076  32.162991   \n",
      "3471   74.013153   -1.175301  -0.919561  2.487262   35.761543  54.516815   \n",
      "3472   74.219727    2.742838  -2.500541 -0.490748   61.085541  38.504871   \n",
      "3473   88.402435    1.353269   1.295585  0.698151   60.944412  40.035755   \n",
      "3474   64.750130    2.202798  -1.061673  0.931861   22.299885  31.729900   \n",
      "3475   66.163574    3.047302  -1.148286 -0.813258   52.885105  17.391628   \n",
      "3476  103.255569    1.498533   0.717593  1.302505   47.340984  26.158401   \n",
      "3477  135.681030    3.092373  -0.688789  0.597197   88.806816  12.868657   \n",
      "3478   59.721073    2.651215  -2.843857  2.439962   37.370964  89.194633   \n",
      "3479  165.691513    2.309832   0.598278  1.250510  107.049438  14.581330   \n",
      "3480   77.532303   -2.838212  -2.695828 -0.901973   69.346115  21.917179   \n",
      "3481   59.300079   -1.416857  -2.201285 -0.455370   54.993443  33.234169   \n",
      "3482  122.805275   -2.360581  -1.081750  0.182641  123.424225  42.747047   \n",
      "3483   21.887432   -1.891810  -0.879248  2.125357    5.774255  69.578629   \n",
      "3484   87.589157   -1.995966  -0.636060  1.070174   54.533878  30.966105   \n",
      "3485  104.272118    2.819062  -0.462706  0.424786   44.953030  80.713699   \n",
      "3486  121.195778    2.436149  -1.141484 -0.102368   81.148941  25.948385   \n",
      "3487   60.638889    1.648786   2.117556 -0.971303   49.023460  28.174973   \n",
      "3488   88.263954   -2.508135   3.080894 -0.989537   84.515854  17.140247   \n",
      "3489   68.749557    2.316120  -2.551850  1.469895   35.417477  26.713572   \n",
      "3490   58.736847    1.172078   0.547339  1.213650   51.080025  23.309317   \n",
      "3491  113.106667    3.021411  -0.819125 -1.434960   33.729275  53.916782   \n",
      "3492   89.181686    2.437683  -0.508738  1.282265   34.763828  33.406464   \n",
      "3493   81.551804   -1.093909  -1.053428 -0.849561   62.286148  63.727802   \n",
      "3494   46.915195    1.271530   0.379163 -1.902625   55.098000  31.024731   \n",
      "\n",
      "      eta_b1b2     pt_b1b2   mass_b1b2  dR_minbl  dR_l1l2b1b2          HT  \\\n",
      "0    -2.909359   15.684971   91.713562  1.162652     2.476826  217.890778   \n",
      "1    -1.151155  118.762001  134.645676  0.634596     1.554868  484.771271   \n",
      "2    -2.138483   41.654747  104.865074  1.265309     1.227948  367.344788   \n",
      "3     0.336391    9.859756   87.980797  0.492013     1.938174  374.350861   \n",
      "4     0.462490   20.551210  106.631134  0.916587     0.964962  256.079834   \n",
      "5    -1.488719   27.226681  119.270882  1.252994     2.330656  259.984100   \n",
      "6    -2.865613   26.272818  155.646713  2.032969     2.658050  245.188263   \n",
      "7    -0.011660   65.755363  124.911629  0.778666     0.983582  500.968384   \n",
      "8    -0.096063   25.639593   78.780220  0.586393     0.684555  237.127441   \n",
      "9    -2.515050   18.536161   92.520729  0.673381     2.887459  233.649536   \n",
      "10   -0.976964   75.079315  121.895470  0.860832     3.145370  253.464478   \n",
      "11    0.643661  117.854996  142.569260  0.733375     1.081050  626.760742   \n",
      "12   -3.146149   31.661407  107.760872  0.684500     3.384605  223.924835   \n",
      "13   -0.725076   38.196651   90.286079  1.269889     2.975414  263.770721   \n",
      "14   -1.392145   52.057339   82.169662  0.718202     0.459437  517.963196   \n",
      "15   -0.063821   26.248123  113.100594  1.228892     0.597965  349.119141   \n",
      "16   -2.882075    9.369115  124.640602  1.148848     3.241274  213.672256   \n",
      "17   -1.821923   72.071068  137.744568  1.252883     1.449358  557.036438   \n",
      "18   -1.000972   23.759918   93.239731  1.002900     3.202968  426.110535   \n",
      "19   -0.227839   97.965439  117.925926  0.787905     1.829867  430.424866   \n",
      "20    1.217260   55.616711  151.908005  0.851308     1.591303  503.814636   \n",
      "21   -0.746468   44.907505   98.549126  1.043385     2.790064  332.712708   \n",
      "22    0.899233   19.304577   91.342674  0.624592     0.865448  205.678116   \n",
      "23   -1.110051   25.524616  111.821014  0.913834     1.439985  253.726578   \n",
      "24    1.646528   30.503956  178.331238  0.914434     1.404188  400.109161   \n",
      "25    2.092898   45.865868  117.980217  0.684042     1.477153  395.443420   \n",
      "26    0.826606  192.125076  113.372475  0.619499     0.855970  903.718079   \n",
      "27   -0.495100   35.020252   67.754379  1.488804     2.602299  369.197083   \n",
      "28    0.664825   52.806755  164.666840  1.767217     1.764894  375.279297   \n",
      "29   -3.945052   15.699574  111.457619  1.034778     2.396556  252.279022   \n",
      "...        ...         ...         ...       ...          ...         ...   \n",
      "3465  1.739110   52.334202  120.903198  1.386245     2.408340  299.137512   \n",
      "3466  4.623565    6.540273  109.569130  0.884495     3.482155  276.074799   \n",
      "3467 -2.063748   50.985920  102.577797  0.536090     3.080791  405.662231   \n",
      "3468 -1.133015   70.033211   95.302277  0.537220     0.670144  394.631165   \n",
      "3469  1.899583  106.029572  105.971855  2.101583     2.736827  416.307861   \n",
      "3470  1.648693   38.822647  103.776756  0.713031     2.638774  254.019531   \n",
      "3471  1.586300  158.754669  169.463470  1.237892     1.287372  664.454895   \n",
      "3472  0.295167   36.087959  127.406181  1.183306     2.621139  297.392059   \n",
      "3473  0.200574  162.413849  112.749199  0.493911     1.387848  550.687866   \n",
      "3474 -1.417401   44.281487   96.494148  1.671614     2.578019  302.057098   \n",
      "3475 -1.158226   71.274094   96.380638  0.813992     1.198985  407.451324   \n",
      "3476  1.440122  170.667603  132.964935  0.505735     0.730670  787.780151   \n",
      "3477  2.535693   25.785418  104.363449  0.573209     2.057230  327.365662   \n",
      "3478  2.874754   28.945883   96.314278  1.201618     2.876902  363.438019   \n",
      "3479  2.391956   22.090185  101.631226  1.229971     1.288733  475.803436   \n",
      "3480 -0.890321   44.398670  113.212723  1.224602     2.695853  305.489227   \n",
      "3481 -1.340204   30.934385   68.815994  0.615484     2.372464  443.997437   \n",
      "3482 -0.033870   41.853127   98.758934  0.687875     1.103204  562.266174   \n",
      "3483  1.069541   27.725519   91.575256  0.869285     1.373981  281.718414   \n",
      "3484  1.525226   27.248398  105.551949  0.661771     0.782078  292.645508   \n",
      "3485  2.050722   33.243999   91.851601  1.225504     1.690492  256.909607   \n",
      "3486 -1.157010   14.916311  102.781197  0.437525     1.554109  272.322815   \n",
      "3487 -3.285670   10.686469  119.821404  1.493061     3.136931  333.917206   \n",
      "3488 -2.135339   29.015221   94.090057  1.851123     3.287061  227.221451   \n",
      "3489  4.008278   10.838016   93.490028  0.681356     3.599351  214.804565   \n",
      "3490  0.927052   53.823753  109.475372  1.210351     0.617834  311.665771   \n",
      "3491 -1.066153   47.960445  115.161346  0.630359     0.898323  286.061584   \n",
      "3492 -2.114422   30.763008  157.963791  1.811123     3.434573  297.456665   \n",
      "3493 -1.091279   66.140274   67.199043  0.924729     1.080805  467.499420   \n",
      "3494 -2.525023   22.315601   57.427116  0.484423     0.728796  323.775665   \n",
      "\n",
      "          met_pt  muon1_pogSF  muon2_pogSF    XsecBr  fin_weight  mass  \n",
      "0      22.374416     0.983626     0.991341  0.121401    0.118379   260  \n",
      "1      62.862717     0.898576     0.979628  0.121401    0.106866   260  \n",
      "2      43.135483     0.977344     0.996623  0.121401    0.118250   260  \n",
      "3      55.579906     0.967682     0.968519  0.121401    0.113779   260  \n",
      "4      49.145042     0.967231     0.997454  0.121401    0.117124   260  \n",
      "5      38.671242     0.967693     0.996912  0.121401    0.117116   260  \n",
      "6      68.266739     0.979950     0.978783  0.121401    0.116443   260  \n",
      "7     101.477745     0.967218     0.969368  0.121401    0.113824   260  \n",
      "8      98.169197     0.967573     0.996954  0.121401    0.117106   260  \n",
      "9      45.628826     0.949588     0.997455  0.121401    0.114987   260  \n",
      "10     30.967896     0.966395     0.979182  0.121401    0.114879   260  \n",
      "11     53.553837     0.965824     0.997060  0.121401    0.116907   260  \n",
      "12     49.643845     0.948149     0.996849  0.121401    0.114743   260  \n",
      "13     73.509781     0.959595     0.980059  0.121401    0.114173   260  \n",
      "14     36.319984     0.983500     0.981921  0.121401    0.117239   260  \n",
      "15     45.452740     0.978208     0.996748  0.121401    0.118369   260  \n",
      "16     48.285545     0.981977     0.997559  0.121401    0.118922   260  \n",
      "17     90.074821     0.897936     0.997488  0.121401    0.108736   260  \n",
      "18     23.718023     0.969115     0.968518  0.121401    0.113947   260  \n",
      "19     82.402313     0.969691     0.979239  0.121401    0.115277   260  \n",
      "20     20.768082     0.861054     0.996558  0.121401    0.104173   260  \n",
      "21     53.665874     0.947107     0.995181  0.121401    0.114425   260  \n",
      "22     49.048340     0.979979     0.997161  0.121401    0.118632   260  \n",
      "23     59.185326     0.968482     0.997549  0.121401    0.117286   260  \n",
      "24     70.778847     0.966974     0.959916  0.121401    0.112686   260  \n",
      "25     43.041447     0.950363     0.959228  0.121401    0.110671   260  \n",
      "26     30.368019     0.987519     0.962353  0.121401    0.115372   260  \n",
      "27     36.185764     0.980548     0.997773  0.121401    0.118774   260  \n",
      "28     67.271858     0.960362     0.962283  0.121401    0.112191   260  \n",
      "29     36.169933     0.978675     1.017113  0.121401    0.120845   260  \n",
      "...          ...          ...          ...       ...         ...   ...  \n",
      "3465   87.824806     0.970048     0.998509  0.121401    0.117589   260  \n",
      "3466   52.598835     0.922323     0.977896  0.121401    0.109496   260  \n",
      "3467   56.674431     0.983762     0.977046  0.121401    0.116688   260  \n",
      "3468   92.558121     0.969213     0.994231  0.121401    0.116984   260  \n",
      "3469   41.056583     0.985310     1.000000  0.121401    0.119617   260  \n",
      "3470   33.585457     0.960611     0.997529  0.121401    0.116331   260  \n",
      "3471  124.590118     0.981720     0.995073  0.121401    0.118594   260  \n",
      "3472   23.464952     0.967228     0.979187  0.121401    0.114978   260  \n",
      "3473   81.761063     0.966882     0.997033  0.121401    0.117032   260  \n",
      "3474   59.094051     0.979350     0.997425  0.121401    0.118588   260  \n",
      "3475   20.740068     0.969139     0.979170  0.121401    0.115204   260  \n",
      "3476  121.368317     0.980575     0.998548  0.121401    0.118870   260  \n",
      "3477   51.855522     0.978220     0.960584  0.121401    0.114076   260  \n",
      "3478   25.353359     0.984152     0.948038  0.121401    0.113269   260  \n",
      "3479   76.619759     0.987104     0.947837  0.121401    0.113584   260  \n",
      "3480   22.177616     0.947236     0.969220  0.121401    0.111456   260  \n",
      "3481   37.762115     0.969234     0.958897  0.121401    0.112829   260  \n",
      "3482   35.723694     0.978036     0.965794  0.121401    0.114673   260  \n",
      "3483   31.532852     0.968872     0.970084  0.121401    0.114103   260  \n",
      "3484   49.799339     0.986532     0.997554  0.121401    0.119473   260  \n",
      "3485   62.067070     0.978060     0.978461  0.121401    0.116180   260  \n",
      "3486   51.383408     0.976865     0.979196  0.121401    0.116125   260  \n",
      "3487   34.792496     0.963618     0.978626  0.121401    0.114484   260  \n",
      "3488   25.520210     0.949666     0.969206  0.121401    0.111740   260  \n",
      "3489   39.759960     0.981463     0.996268  0.121401    0.118706   260  \n",
      "3490   55.202293     0.969888     0.995014  0.121401    0.117158   260  \n",
      "3491   95.165298     0.947311     0.984193  0.121401    0.113186   260  \n",
      "3492   64.909683     0.934718     0.977779  0.121401    0.110954   260  \n",
      "3493   98.687302     0.976991     0.995827  0.121401    0.118113   260  \n",
      "3494   28.324392     0.985489     0.992431  0.121401    0.118734   260  \n",
      "\n",
      "[3495 rows x 18 columns]\n",
      "The shape for hh is (samples, features): \n",
      "(3495, 18)\n",
      "The shape for tt is (samples, features): \n",
      "(1891414, 18)\n",
      "Index([u'mass_trans', u'dphi_llmet', u'dphi_llbb', u'eta_l1l2', u'pt_l1l2',\n",
      "       u'mass_l1l2', u'eta_b1b2', u'pt_b1b2', u'mass_b1b2', u'dR_minbl',\n",
      "       u'dR_l1l2b1b2', u'HT', u'met_pt', u'muon1_pogSF', u'muon2_pogSF',\n",
      "       u'XsecBr', u'fin_weight', u'mass'],\n",
      "      dtype='object')\n",
      "Index([u'mass_trans', u'dphi_llmet', u'dphi_llbb', u'eta_l1l2', u'pt_l1l2',\n",
      "       u'mass_l1l2', u'eta_b1b2', u'pt_b1b2', u'mass_b1b2', u'dR_minbl',\n",
      "       u'dR_l1l2b1b2', u'HT', u'met_pt', u'muon1_pogSF', u'muon2_pogSF',\n",
      "       u'XsecBr', u'fin_weight', u'mass'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print \"Now let's add a parameter in each df that represet the mass\"\n",
    "HH_ALL[0]['mass'] = 260 \n",
    "HH_ALL[1]['mass'] = 270 \n",
    "HH_ALL[2]['mass'] = 300 \n",
    "HH_ALL[3]['mass'] = 350 \n",
    "HH_ALL[4]['mass'] = 400 \n",
    "HH_ALL[5]['mass'] = 450 \n",
    "HH_ALL[6]['mass'] = 500 \n",
    "HH_ALL[7]['mass'] = 550 \n",
    "HH_ALL[8]['mass'] = 600 \n",
    "HH_ALL[9]['mass'] = 650 \n",
    "HH_ALL[10]['mass'] = 750 \n",
    "HH_ALL[11]['mass'] = 850 \n",
    "HH_ALL[12]['mass'] = 900 \n",
    "HH_ALL[13]['mass'] = 1000\n",
    "ttbar['mass'] = 0\n",
    "\n",
    "if debug:\n",
    "    print(\"---> HH_ALL[0] Displayed as panda dataframe: \"); print(HH_ALL[0])\n",
    "    print(\"The shape for hh is (samples, features): \"); print(HH_ALL[0].shape)\n",
    "    print(\"The shape for tt is (samples, features): \"); print(ttbar.shape)\n",
    "    print HH_ALL[0].keys()\n",
    "    print ttbar.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline #Too many plots to dispal them here\n",
    "# Plots of the branches we selected\n",
    "if MakePlots:\n",
    "    iH =0\n",
    "    for HH in HH_ALL:\n",
    "        for key in ttbar.keys() :\n",
    "            if(key!=\"muon1_pogSF\" and key!=\"muon2_pogSF\" and key!=\"XsecBr\" and key!=\"fin_weight\") :\n",
    "                matplotlib.rcParams.update({'font.size': 16})\n",
    "                fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "                bins = np.linspace(my_max(min(ttbar[key]),0.), max(ttbar[key]), 50)\n",
    "                _ = plt.hist(HH[key],  bins=bins, histtype='step', normed=True, label=sampleName[iH], linewidth=2)\n",
    "                _ = plt.hist(ttbar[key], bins=bins, histtype='step', normed=True, label=r'$t\\overline{t}$')\n",
    "                plt.xlabel(key)\n",
    "                plt.ylabel('Entries')\n",
    "                plt.legend(loc='best')\n",
    "                plt.savefig(folder + \"/\" + str(key) + '_' + str(sampleName[iH]) + '.pdf')\n",
    "        iH+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need:\n",
    "1. X : matrix with raw=#Events and column=Variables to discriminate.\"\n",
    "2. w : A vector containig the weights of each event\"\n",
    "3. Y : A vector containing for each event if it is signal (0) or TT (1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Now lets start to talk about DNN!')\n",
    "#You only need a Dataframe for the training. So you merge all the one you have\n",
    "df =  pd.concat((HH_ALL[0][my_branches_training], HH_ALL[1][my_branches_training], HH_ALL[2][my_branches_training], HH_ALL[3][my_branches_training], HH_ALL[4][my_branches_training], HH_ALL[5][my_branches_training], HH_ALL[6][my_branches_training], HH_ALL[7][my_branches_training], HH_ALL[8][my_branches_training], HH_ALL[9][my_branches_training], HH_ALL[10][my_branches_training], HH_ALL[11][my_branches_training], HH_ALL[12][my_branches_training], HH_ALL[13][my_branches_training], ttbar[my_branches_training]), ignore_index=True)\n",
    "# Turn the df the desired ndarray \"X\" that can be directly used for ML applications.\n",
    "X = df.as_matrix() # Each row is an object to classify, each column corresponds to a specific variable.\n",
    "# Take the weights\n",
    "w =  pd.concat((HH_ALL[0]['fin_weight'],HH_ALL[1]['fin_weight'],HH_ALL[2]['fin_weight'],HH_ALL[3]['fin_weight'],HH_ALL[4]['fin_weight'],HH_ALL[5]['fin_weight'],HH_ALL[6]['fin_weight'],HH_ALL[7]['fin_weight'],HH_ALL[8]['fin_weight'],HH_ALL[9]['fin_weight'],HH_ALL[10]['fin_weight'],HH_ALL[11]['fin_weight'],HH_ALL[12]['fin_weight'],HH_ALL[13]['fin_weight'], ttbar['fin_weight']), ignore_index=True).values\n",
    "# This is the array with the true values: 0 is signal, 1 if TT.\n",
    "y = []\n",
    "for _df, ID in [(HH_ALL[0], 0),(HH_ALL[1], 0),(HH_ALL[2], 0),(HH_ALL[3], 0),(HH_ALL[4], 0),(HH_ALL[5], 0),(HH_ALL[6], 0),(HH_ALL[7], 0),(HH_ALL[8], 0),(HH_ALL[9], 0),(HH_ALL[10], 0),(HH_ALL[11], 0),(HH_ALL[12], 0),(HH_ALL[13], 0), (ttbar, 1)]:\n",
    "    y.extend([ID] * _df.shape[0])\n",
    "y = np.array(y)\n",
    "\n",
    "# Randomly shuffle and automatically split all your objects into train and test subsets\n",
    "ix = range(X.shape[0]) # array of indices, just to keep track of them for safety reasons and future checks\n",
    "X_train, X_test, y_train, y_test, w_train, w_test, ix_train, ix_test = train_test_split(X, y, w, ix, train_size=0.7) # Train here is 70% of the total statistic\n",
    "# It is common practice to scale the inputs to Neural Nets such that they have approximately similar ranges (it atually improve the results)\n",
    "scaler = StandardScaler() \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test) # You are applying the same transformation done to X_train, to X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron (MLP) definition\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=X_train.shape[1], activation='relu')) # Linear transformation of the input vector. The first number is output_dim.\n",
    "model.add(Dropout(0.1)) # To avoid overfitting. It masks the outputs of the previous layer such that some of them will randomly become inactive and will not contribute to information propagation.\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20, activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10,activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20, activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='softmax')) # Last layer has to have the same dimensionality as the number of classes we want to predict, here 2.\n",
    "model.summary()\n",
    "# Now you need to declare what loss function and optimizer to use (and compile your model).\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('---------------------------Training:---------------------------')\n",
    "try:\n",
    "    history = model.fit(X_train, y_train, batch_size=50, epochs=100, verbose=1,\n",
    "              callbacks = [\n",
    "                  EarlyStopping(verbose=True, patience=6, monitor='val_loss'),\n",
    "                  ModelCheckpoint('models/tutorial-progress.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "              ],\n",
    "              validation_split=0.2, validation_data=None, shuffle=True,\n",
    "              class_weight={\n",
    "                0 : compute_class_weight(\"balanced\", [0, 1], y)[0], # Function that return \"[1/N_classes * ((float(len(y)) / (y == 0).sum())), 1/N_classes * ((float(len(y)) / (y == 1).sum()))]\"\n",
    "                1 : compute_class_weight(\"balanced\", [0, 1], y)[1]\n",
    "              },\n",
    "              sample_weight=None,initial_epoch=0)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print 'Training ended early.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some plots about the convergence of our model. You can check if:\n",
    "#  1. It’s speed of convergence over epochs (slope).\n",
    "#  2. Whether the model may have already converged (plateau of the line).\n",
    "#  3. Whether the mode may be over-learning the training data (inflection for validation line).\n",
    "\n",
    "if debug:\n",
    "    print \"All data in hisotry are: \",history.history.keys()\n",
    "# Summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig(folder + '/Check_accuracy.pdf')\n",
    "# Summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig(folder + '/Check_loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best network (by default you return the last one, you if you save every time you have a better one you are fine loading it later)\n",
    "model.load_weights('./models/tutorial-progress.h5')\n",
    "print 'Saving weights...'\n",
    "model.save_weights('./models/tutorial.h5', overwrite=True)\n",
    "json_string = model.to_json()\n",
    "open('./models/tutorial.json', 'w').write(json_string)\n",
    "print 'Testing...'\n",
    "yhat = model.predict(X_test, verbose = True, batch_size = 50) # Return a vector of 2 indeces [probToBe_S,probToBe_B]\n",
    "#Turn them into classes\n",
    "yhat_cls = np.argmax(yhat, axis=1) # Transform [probToBe_S,probToBe_B] in a vector of 0 and 1 depending if probToBe_S>probToBe_B. Practically return the index of the biggest element (0 is is probToBe_S, if is probToBe_B)\n",
    "# This should Normalized to the Xsec?\n",
    "if MakePlots:\n",
    "    bins = np.linspace(-0.5,1.5,3)\n",
    "    names = ['','','','hh','','','','tt']\n",
    "    fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "    ax = plt.subplot()\n",
    "    ax.set_xticklabels(names, rotation=45)\n",
    "    _ = plt.hist(yhat_cls, bins=bins, histtype='stepfilled', alpha=0.5, label='prediction',log=True)#, weights=w_test)\n",
    "    _ = plt.hist(y_test, bins=bins, histtype='stepfilled', alpha=0.5, label='truth',log=True)#, weights=w_test)\n",
    "    plt.legend(loc='upper right')\n",
    "    print('Saving:',folder + '/Performance.pdf')\n",
    "    plt.savefig(folder + '/Performance.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With \"(y_test != 0) & (yhat_cls == 0)\" you get an arrate of bool: [False False False ..., False False False]\n",
    "print 'Signal efficiency:',     w_test[(y_test == 0) & (yhat_cls == 0)].sum() / w_test[y_test == 0].sum()\n",
    "print 'Background efficiency:', w_test[(y_test != 0) & (yhat_cls == 0)].sum() / w_test[y_test != 0].sum()\n",
    "w_1 = np.ones(len(y_test))\n",
    "print 'Signal efficiency (not weighted):',     w_1[(y_test == 0) & (yhat_cls == 0)].sum() / w_1[y_test == 0].sum()\n",
    "print 'Background efficiency (not weighted):', w_1[(y_test != 0) & (yhat_cls == 0)].sum() / w_1[y_test != 0].sum()\n",
    "print \"Let's compare with the training samples:\"\n",
    "w_1 = np.ones(len(y_train))\n",
    "yhat_tr = model.predict(X_train, verbose = True, batch_size = 50)\n",
    "yhat_trcls = np.argmax(yhat_tr, axis=1)\n",
    "print ''; print 'Signal efficiency (not weighted) for training:',     w_1[(y_train == 0) & (yhat_trcls == 0)].sum() / w_1[y_train == 0].sum()\n",
    "print 'Background efficiency (not weighted) for training:', w_1[(y_train != 0) & (yhat_trcls == 0)].sum() / w_1[y_train != 0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Signal efficiency: 0.949929225605\n",
    "Background efficiency: 0.0542005451175\n",
    "Signal efficiency (not weighted): 0.9499572284\n",
    "Background efficiency (not weighted): 0.0540373349501\n",
    "Let's compare with the training samples:\n",
    "1324550/1329395 [============================>.] - ETA: 0ss \n",
    "Signal efficiency (not weighted) for training: 0.962116991643\n",
    "Background efficiency (not weighted) for training: 0.0539014055785"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
