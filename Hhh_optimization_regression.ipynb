{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS and TENSORFLOW\n",
    "We start by importing all the necessary packages\n",
    "(%lsmagic for magic commanda and ! for bash instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "require(['notebook'],\n",
       "  function() {\n",
       "    IPython.CodeCell.config_defaults.highlight_modes['magic_text/x-c++src'] = {'reg':[/^%%cpp/]};\n",
       "    console.log(\"JupyROOT - %%cpp magic configured\");\n",
       "  }\n",
       ");\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.08/06\n",
      "I you had no error so far, this is great! We can start the tutorial."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Luca2/Library/Python/2.7/lib/python/site-packages/root_numpy/__init__.py:34: RuntimeWarning: ROOT 6.08/06 is currently active but you installed root_numpy against ROOT 6.08/00. Please consider reinstalling root_numpy for this ROOT version.\n",
      "  RuntimeWarning)\n",
      "/Users/Luca2/Library/Python/2.7/lib/python/site-packages/root_numpy/__init__.py:46: RuntimeWarning: numpy 1.12.1 is currently installed but you installed root_numpy against numpy 1.11.0. Please consider reinstalling root_numpy for this numpy version.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys;\n",
    "import os, sys, array, re, math, random, subprocess, glob\n",
    "from math import *\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy.lib.recfunctions import stack_arrays\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Highway, MaxoutDense, Masking, GRU, Merge, Input, merge\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import deepdish.io as io\n",
    "import ROOT\n",
    "from ROOT import gSystem, gROOT, gApplication, TFile, TTree, TCut, TH1F, TCanvas\n",
    "from root_numpy import root2array \n",
    "from IPython.display import HTML, IFrame\n",
    "import seaborn as sns; sns.set()\n",
    "print \"I you had no error so far, this is great! We can start the tutorial.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set some variables we will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check this file out, it contains many functions we will use.\n",
    "execfile(\"Useful_func.py\")\n",
    "# Fix random seed for reproducibility\n",
    "seed = 7; np.random.seed(seed);\n",
    "# Input paramters\n",
    "doSubLead = False # Choose if to do regression on Leading or Subleading Jets\n",
    "debug = True #(Verbose output)\n",
    "folder='Plots_Regression/' # Folder with Plots\n",
    "MakePlots=False # Set False if you want to run faster\n",
    "folderCreation  = subprocess.Popen(['mkdir -p ' + folder], stdout=subprocess.PIPE, shell=True); folderCreation.communicate()\n",
    "folderCreation2 = subprocess.Popen(['mkdir -p models/'], stdout=subprocess.PIPE, shell=True); folderCreation2.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now We Start manipulating ROOT files into a format we can use to train a MVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> hh Displayed as panda dataframe: \n",
      "      numOfVertices    b1jet_pt  b1jet_eta    b1jet_mt  b1jet_leadTrackPt  \\\n",
      "0                19  173.124298  -0.714916  173.738297          47.906250   \n",
      "1                33  246.682297   0.298446  248.075989          58.843750   \n",
      "2                30  340.403229   1.258016  342.380005          98.562500   \n",
      "3                14   80.552818   2.200674   81.073486          22.250000   \n",
      "4                16   86.618324  -1.492352   87.986191          10.437500   \n",
      "5                29  147.646530   0.765988  148.593613          38.468750   \n",
      "6                20  179.774414  -0.576904  181.731323          33.406250   \n",
      "7                13  160.337036  -0.464420  162.175751          36.437500   \n",
      "8                16   67.824074   1.615699   68.589928          27.625000   \n",
      "9                13  118.275604  -1.762509  119.216217          32.031250   \n",
      "10               42  174.260849   1.073892  176.100327          26.484375   \n",
      "11               33  142.049393   2.365291  142.999451          32.187500   \n",
      "12               12  124.097084  -1.880355  124.704475          38.593750   \n",
      "13               12  143.997528   0.601616  144.939636          24.234375   \n",
      "14               18   71.668770  -0.621273   73.713371          11.593750   \n",
      "15               17  168.491440  -0.539859  171.719299          32.968750   \n",
      "16               19   73.634766   0.321159   74.633331          17.312500   \n",
      "17               21  119.909294  -1.196250  120.385368          49.750000   \n",
      "18               23   61.311272   1.524936   62.131676          10.976562   \n",
      "19               16   61.002487   0.815358   62.718700           9.234375   \n",
      "20               19  139.507935  -1.512107  141.882187          27.843750   \n",
      "21               28  110.306679  -0.054426  111.114868          26.453125   \n",
      "22               24  104.096039   1.401750  105.602226          18.250000   \n",
      "23               25  291.652802   0.259874  292.595581         127.875000   \n",
      "24               19   81.992393   1.780108   82.384354          52.343750   \n",
      "25               30  142.617386  -2.139359  145.040131          16.468750   \n",
      "26               23  116.806374   1.657782  117.575821          19.390625   \n",
      "27               27  154.985306  -0.641958  155.757401          39.375000   \n",
      "28               13  165.338593  -0.083306  166.437698          73.812500   \n",
      "29               13   92.013832  -1.531184   93.055374          18.718750   \n",
      "...             ...         ...        ...         ...                ...   \n",
      "4179             10  106.920563   1.595005  107.600555          33.718750   \n",
      "4180             14  168.977829   0.369030  169.942062          20.859375   \n",
      "4181             17  162.309174  -0.738977  163.393280          50.562500   \n",
      "4182             18  195.396683   0.986165  197.111237          51.656250   \n",
      "4183             15  145.113007   0.869500  145.971710          42.187500   \n",
      "4184             21  132.413589  -0.916839  133.690887          23.671875   \n",
      "4185              9  156.645050  -0.551639  157.811218          30.000000   \n",
      "4186             14   64.592789   0.469875   65.037506          26.578125   \n",
      "4187             13   91.033875  -1.366691   91.870544          13.710938   \n",
      "4188             19  208.564087  -0.688823  209.781982          39.437500   \n",
      "4189             13  131.006027  -0.635232  131.669830          38.250000   \n",
      "4190             19  290.065521   0.494386  294.082397          70.250000   \n",
      "4191             14  138.806778  -1.244819  139.321594          42.781250   \n",
      "4192             17   73.083168  -0.811610   73.370872          18.265625   \n",
      "4193             11   89.882904   0.667456   90.569321          19.734375   \n",
      "4194             23  146.358154  -1.029058  147.400726          36.937500   \n",
      "4195             15   92.057373  -0.101632   92.881844          28.609375   \n",
      "4196             17  104.442467  -0.505933  105.075981          24.078125   \n",
      "4197             22   97.425247  -1.337259   98.079651          36.718750   \n",
      "4198             17   53.973141   0.510759   54.886993           6.984375   \n",
      "4199             22  252.435120   0.119302  253.330368          49.406250   \n",
      "4200             21   34.874130   1.170465   35.407772           8.390625   \n",
      "4201             15   72.169090   1.631212   72.741409          10.757812   \n",
      "4202             16  112.309776   2.326932  114.263672          20.156250   \n",
      "4203             18  215.564163  -1.329550  216.930481          38.218750   \n",
      "4204             10  183.669724  -0.545932  184.282684          32.468750   \n",
      "4205             24  142.690216  -0.111881  145.287308          18.406250   \n",
      "4206             24  194.084244  -1.529882  195.278931          42.531250   \n",
      "4207             20   58.787407   1.038611   59.727310          12.554688   \n",
      "4208             24  205.237549   0.429516  206.807175          67.312500   \n",
      "\n",
      "      b1jet_leptonDeltaR  b1jet_leptonPtRel  b1jet_leptonPt  b1jet_vtxPt  \\\n",
      "0               8.886246           0.000000       -1.000000    74.793633   \n",
      "1               9.475537           0.000000       -1.000000     0.000000   \n",
      "2              10.648453           0.000000       -1.000000    32.354179   \n",
      "3               0.030109           0.616346       20.328125     0.000000   \n",
      "4               8.461505           0.000000       -1.000000     0.000000   \n",
      "5              10.325735           0.000000       -1.000000    51.358002   \n",
      "6               8.449391           0.000000       -1.000000    59.352242   \n",
      "7               8.993304           0.000000       -1.000000    39.031902   \n",
      "8              11.567269           0.000000       -1.000000     9.160653   \n",
      "9               0.240124           1.074758        4.089844    36.400349   \n",
      "10             11.049647           0.000000       -1.000000    37.085175   \n",
      "11              0.345360           0.914004        2.976562    74.579536   \n",
      "12              7.274754           0.000000       -1.000000    74.031311   \n",
      "13             10.610615           0.000000       -1.000000    46.212391   \n",
      "14              0.192709           2.019463       10.992188    24.498207   \n",
      "15              0.196314           5.027470       24.781250    31.539532   \n",
      "16              0.145149           1.424389        9.976562    21.788664   \n",
      "17              8.662917           0.000000       -1.000000    26.808952   \n",
      "18             11.824152           0.000000       -1.000000    22.394354   \n",
      "19              0.208740           1.845764        9.234375    16.530825   \n",
      "20              0.080805           0.395285        4.757812    33.233883   \n",
      "21              9.439346           0.000000       -1.000000    39.979744   \n",
      "22              0.065805           0.926004       14.359375    22.372431   \n",
      "23              9.902719           0.000000       -1.000000    72.094505   \n",
      "24              0.022394           1.172604       52.343750     0.000000   \n",
      "25              8.440980           0.000000       -1.000000    48.842617   \n",
      "26              0.024754           0.316189       12.851562    28.930849   \n",
      "27              8.358043           0.000000       -1.000000   112.758125   \n",
      "28              0.130784           0.720070        5.515625    10.892419   \n",
      "29              7.470647           0.000000       -1.000000     0.000000   \n",
      "...                  ...                ...             ...          ...   \n",
      "4179            0.028555           0.975030       33.718750    13.896862   \n",
      "4180           10.113488           0.000000       -1.000000    35.457756   \n",
      "4181            8.778753           0.000000       -1.000000    29.944033   \n",
      "4182           10.057064           0.000000       -1.000000     0.000000   \n",
      "4183           10.051836           0.000000       -1.000000    67.497658   \n",
      "4184            8.448045           0.000000       -1.000000    18.176243   \n",
      "4185            0.018033           0.542842       30.000000    32.532211   \n",
      "4186            0.008087           0.214524       26.578125     0.000000   \n",
      "4187            0.045076           0.336845        7.582031    44.351162   \n",
      "4188            8.950663           0.000000       -1.000000     0.000000   \n",
      "4189            9.048704           0.000000       -1.000000    55.797474   \n",
      "4190           10.442945           0.000000       -1.000000    58.515644   \n",
      "4191            0.079511           0.883417       11.085938    80.296387   \n",
      "4192            8.420338           0.000000       -1.000000    31.049994   \n",
      "4193            9.693789           0.000000       -1.000000    26.811209   \n",
      "4194            9.134704           0.000000       -1.000000    62.402737   \n",
      "4195            9.511589           0.000000       -1.000000     0.000000   \n",
      "4196            8.527472           0.000000       -1.000000    63.183777   \n",
      "4197            9.638962           0.000000       -1.000000    49.043892   \n",
      "4198            9.727963           0.000000       -1.000000    26.755766   \n",
      "4199            9.206501           0.000000       -1.000000   158.867340   \n",
      "4200            0.074485           0.643987        8.390625    13.533996   \n",
      "4201            0.012889           0.137996       10.757812    39.250118   \n",
      "4202           11.417752           0.000000       -1.000000     0.000000   \n",
      "4203            0.143067           0.792131        5.882812    55.851654   \n",
      "4204            9.968075           0.000000       -1.000000    62.366951   \n",
      "4205            8.903337           0.000000       -1.000000    24.979242   \n",
      "4206            7.471152           0.000000       -1.000000    21.232380   \n",
      "4207           10.287676           0.000000       -1.000000    37.401752   \n",
      "4208            9.606818           0.000000       -1.000000    88.483246   \n",
      "\n",
      "      b1jet_vtxMass  b1jet_vtxNtracks  b1jet_neHEF  b1jet_neEmEF  \\\n",
      "0          1.621181               5.0     0.050416      0.484596   \n",
      "1          0.000000               0.0     0.138334      0.352227   \n",
      "2          0.974425               2.0     0.008021      0.458380   \n",
      "3          0.000000               0.0     0.098938      0.361311   \n",
      "4          0.000000               0.0     0.081685      0.191647   \n",
      "5          1.497231               4.0     0.000000      0.416437   \n",
      "6          1.739681               5.0     0.032811      0.386302   \n",
      "7          2.327658               7.0     0.063804      0.437253   \n",
      "8          1.250745               4.0     0.014668      0.188212   \n",
      "9          1.130635               2.0     0.052190      0.266156   \n",
      "10         0.912831               3.0     0.057075      0.405371   \n",
      "11         2.521918               6.0     0.036332      0.330625   \n",
      "12         2.496731               5.0     0.109807      0.068393   \n",
      "13         1.682473               4.0     0.056433      0.471207   \n",
      "14         3.971229               4.0     0.068335      0.153292   \n",
      "15         1.394474               2.0     0.092126      0.104681   \n",
      "16         1.927003               3.0     0.125832      0.456496   \n",
      "17         1.102442               2.0     0.466162      0.208322   \n",
      "18         0.993611               4.0     0.140550      0.309787   \n",
      "19         2.311434               3.0     0.017308      0.294034   \n",
      "20         0.787441               2.0     0.074661      0.264439   \n",
      "21         1.805104               3.0     0.114604      0.424745   \n",
      "22         0.865642               2.0     0.173671      0.134720   \n",
      "23         3.475206               6.0     0.025497      0.533626   \n",
      "24         0.000000               0.0     0.026820      0.085760   \n",
      "25         2.747471               8.0     0.060475      0.125024   \n",
      "26         1.147392               2.0     0.068261      0.046258   \n",
      "27         3.618078               9.0     0.035768      0.170569   \n",
      "28         0.922683               3.0     0.042418      0.633090   \n",
      "29         0.000000               0.0     0.019250      0.427386   \n",
      "...             ...               ...          ...           ...   \n",
      "4179       0.787464               2.0     0.000000      0.063075   \n",
      "4180       1.646550               4.0     0.000000      0.084273   \n",
      "4181       2.206645               4.0     0.021163      0.497783   \n",
      "4182       0.000000               0.0     0.058190      0.487911   \n",
      "4183       2.562554               4.0     0.035697      0.176890   \n",
      "4184       0.754159               3.0     0.195004      0.270035   \n",
      "4185       0.721098               3.0     0.198016      0.294027   \n",
      "4186       0.000000               0.0     0.145080      0.176597   \n",
      "4187       2.464221               4.0     0.011300      0.222361   \n",
      "4188       0.000000               0.0     0.064089      0.355883   \n",
      "4189       3.461878               7.0     0.000000      0.363505   \n",
      "4190       1.418736               4.0     0.270883      0.353508   \n",
      "4191       2.654532               3.0     0.009667      0.244694   \n",
      "4192       1.733792               4.0     0.058730      0.194406   \n",
      "4193       2.271355               4.0     0.140116      0.445143   \n",
      "4194       2.253494               3.0     0.050462      0.122176   \n",
      "4195       0.000000               0.0     0.072155      0.356835   \n",
      "4196       2.896321               4.0     0.054089      0.115826   \n",
      "4197       3.103243               4.0     0.148426      0.204084   \n",
      "4198       3.157123               7.0     0.127100      0.228040   \n",
      "4199       2.972229               5.0     0.036805      0.272501   \n",
      "4200       1.520187               4.0     0.078941      0.100557   \n",
      "4201       2.054717               5.0     0.007880      0.166400   \n",
      "4202       0.000000               0.0     0.000000      0.455082   \n",
      "4203       1.523466               3.0     0.079329      0.192642   \n",
      "4204       2.829094               6.0     0.000000      0.185033   \n",
      "4205       2.729821               4.0     0.051243      0.215653   \n",
      "4206       0.376182               2.0     0.177323      0.331266   \n",
      "4207       4.190404               6.0     0.093170      0.147334   \n",
      "4208       2.671451               3.0     0.079875      0.287074   \n",
      "\n",
      "      b1jet_vtx3DSig  b1jet_vtx3DVal  b1genjet_pt    target  \n",
      "0          26.157988        1.967698   172.178650  0.994538  \n",
      "1           0.000000        0.000000   195.914536  0.794198  \n",
      "2           7.705590        0.944695   304.522736  0.894594  \n",
      "3           0.000000        0.000000    54.073444  0.671279  \n",
      "4           0.000000        0.000000    81.036469  0.935558  \n",
      "5          10.634332        0.517075   160.419449  1.086510  \n",
      "6           7.832742        0.336888   190.524002  1.059795  \n",
      "7         111.891029        1.903939   145.027847  0.904519  \n",
      "8          15.613366        1.055085    40.859528  0.602434  \n",
      "9           4.221067        0.729125    29.835064  0.252250  \n",
      "10         59.300240        3.875120    25.439190  0.145983  \n",
      "11          9.813640        2.269392    54.086449  0.380758  \n",
      "12         12.895526        1.508751   148.629974  1.197691  \n",
      "13         70.620102        2.019354   142.146790  0.987147  \n",
      "14          6.774051        0.119047    77.364563  1.079474  \n",
      "15         36.233181        1.567695    38.814510  0.230365  \n",
      "16         34.668743        0.558064    75.007530  1.018643  \n",
      "17          4.029860        0.370166   112.457726  0.937857  \n",
      "18         12.429051        1.608558   103.063805  1.680993  \n",
      "19         14.351771        0.375071    26.262325  0.430512  \n",
      "20         35.490208        3.891939   160.244614  1.148642  \n",
      "21         25.943197        0.521567    96.823601  0.877767  \n",
      "22          7.299679        0.716665   196.157196  1.884387  \n",
      "23          6.141870        0.121861    58.760139  0.201473  \n",
      "24          0.000000        0.000000    94.045120  1.146998  \n",
      "25         95.135391        9.070968   140.353271  0.984125  \n",
      "26         11.310946        1.180041   100.526291  0.860623  \n",
      "27         95.211067        2.471222   146.179962  0.943186  \n",
      "28         29.623516        0.763060    34.241428  0.207099  \n",
      "29          0.000000        0.000000    66.349541  0.721082  \n",
      "...              ...             ...          ...       ...  \n",
      "4179       80.747337        5.091310    97.303169  0.910051  \n",
      "4180       12.049495        0.340086    63.751541  0.377278  \n",
      "4181       65.610970        1.475026    18.061647  0.111279  \n",
      "4182        0.000000        0.000000   168.360474  0.861634  \n",
      "4183      137.901337        3.142104    55.107250  0.379754  \n",
      "4184       28.632401        3.441884    98.093506  0.740811  \n",
      "4185       20.371883        2.223397    72.392227  0.462142  \n",
      "4186        0.000000        0.000000    43.591057  0.674860  \n",
      "4187        9.091996        0.358959   108.653152  1.193546  \n",
      "4188        0.000000        0.000000    36.969624  0.177258  \n",
      "4189       32.269066        0.708640    78.951546  0.602656  \n",
      "4190       62.424313        2.307542    76.908180  0.265141  \n",
      "4191       48.107204        2.382466    41.935692  0.302116  \n",
      "4192       28.372293        0.905632    51.823368  0.709101  \n",
      "4193      105.291374        1.804788    76.075172  0.846381  \n",
      "4194       86.123398        3.343389   145.684204  0.995395  \n",
      "4195        0.000000        0.000000    72.891304  0.791803  \n",
      "4196       24.550695        0.510523    94.095932  0.900936  \n",
      "4197       86.534584        2.708078    99.048744  1.016664  \n",
      "4198       24.108347        0.302249    43.435471  0.804761  \n",
      "4199       31.803873        1.445006    40.416618  0.160107  \n",
      "4200        9.507304        0.457221    34.610790  0.992449  \n",
      "4201       39.083225        2.107584    84.251884  1.167423  \n",
      "4202        0.000000        0.000000    38.743317  0.344968  \n",
      "4203       83.166000        4.362824    69.819923  0.323894  \n",
      "4204       17.599173        0.385763    73.018883  0.397555  \n",
      "4205       12.401098        0.151244    87.586975  0.613826  \n",
      "4206        6.663843        4.151495    23.140509  0.119229  \n",
      "4207       32.566494        0.527393    52.341438  0.890351  \n",
      "4208       20.452982        0.582686    75.035706  0.365604  \n",
      "\n",
      "[4209 rows x 17 columns]\n",
      "The shape for LeadJet is (samples, features): \n",
      "(4209, 17)\n",
      "The shape for SubLeadJet is (samples, features): \n",
      "(4209, 17)\n"
     ]
    }
   ],
   "source": [
    "# Our goal is to separate BX Signal from TT background. The machine learning only want to analyze \"good events\": \n",
    "# 1) Applying a preselection to out Signal and background events\n",
    "my_selec_jet1 = 'b1genjet_pt>0 && met_pt>20 && met_pt<500 && muon1_pt>20 && fabs(muon1_eta)<2.4 && pt_l1l2<500 && pt_b1b2<500 && mass_l1l2>12 && mass_l1l2<500 && mass_b1b2<500 && b1jet_pt>20 && fabs(b1jet_eta)<2.4 && b2jet_pt>20 && fabs(b2jet_eta)<2.4 && mass_trans>10 && mass_trans<500 && HT<4000'\n",
    "my_selec_jet2 = 'b2genjet_pt>0 && met_pt>20 && met_pt<500 && muon2_pt>10 && fabs(muon2_eta)<2.4 && pt_l1l2<500 && pt_b1b2<500 && mass_l1l2>12 && mass_l1l2<500 && mass_b1b2<500 && b1jet_pt>20 && fabs(b1jet_eta)<2.4 && b2jet_pt>20 && fabs(b2jet_eta)<2.4 && mass_trans>10 && mass_trans<500 && HT<4000'\n",
    "# 2) Selecting the branches that contains the information we want to use (in general)\n",
    "my_branches_jet1 = [\"numOfVertices\",\"b1jet_pt\",\"b1jet_eta\",\"b1jet_mt\",\"b1jet_leadTrackPt\",\"b1jet_leptonDeltaR\",\"b1jet_leptonPtRel\",\"b1jet_leptonPt\",\"b1jet_vtxPt\",\"b1jet_vtxMass\",\"b1jet_vtxNtracks\",\"b1jet_neHEF\",\"b1jet_neEmEF\",\"b1jet_vtx3DSig\",\"b1jet_vtx3DVal\",\"b1genjet_pt\"]\n",
    "my_branches_jet2 = [\"numOfVertices\",\"b2jet_pt\",\"b2jet_eta\",\"b2jet_mt\",\"b2jet_leadTrackPt\",\"b2jet_leptonDeltaR\",\"b2jet_leptonPtRel\",\"b2jet_leptonPt\",\"b2jet_vtxPt\",\"b2jet_vtxMass\",\"b2jet_vtxNtracks\",\"b2jet_neHEF\",\"b2jet_neEmEF\",\"b2jet_vtx3DSig\",\"b2jet_vtx3DVal\",\"b2genjet_pt\"]\n",
    "# 3) Selecting the branches that contains the information we want to use (in the training)\n",
    "my_branches_training_jet1 = my_branches_jet1\n",
    "my_branches_training_jet2 = my_branches_jet2\n",
    "# Converting Root files in dataframe (Very useful, checnl root2panda in Useful_func.py)\n",
    "LeadJet    = root2panda('files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-500_narrow_13TeV-madgraph-v2.root', 'DiHiggsWWBBAna/evtree', branches=my_branches_jet1, selection=my_selec_jet1)\n",
    "SubLeadJet = root2panda('files/MVA_GluGluToRadionToHHTo2B2VTo2L2Nu_M-500_narrow_13TeV-madgraph-v2.root', 'DiHiggsWWBBAna/evtree', branches=my_branches_jet2, selection=my_selec_jet2)\n",
    "# Define the target you need\n",
    "LeadJet[\"target\"] = LeadJet[\"b1genjet_pt\"]/LeadJet[\"b1jet_pt\"]\n",
    "SubLeadJet[\"target\"] = SubLeadJet[\"b2genjet_pt\"]/SubLeadJet[\"b2jet_pt\"]\n",
    "\n",
    "if debug:\n",
    "  print(\"---> hh Displayed as panda dataframe: \"); print(LeadJet)\n",
    "  print(\"The shape for LeadJet is (samples, features): \"); print(LeadJet.shape)\n",
    "  print(\"The shape for SubLeadJet is (samples, features): \"); print(SubLeadJet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Plots of the branches we selected\n",
    "if MakePlots:\n",
    "  for key in LeadJet.keys() :\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "    fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "    bins = np.linspace(my_max(min(LeadJet[key]),-99.), max(LeadJet[key]), 100)\n",
    "    _ = plt.hist(LeadJet[key],  bins=bins, histtype='step', normed=True, label=r'$Leading Jet$', linewidth=2)\n",
    "    plt.xlabel(key)\n",
    "    plt.ylabel('Entries')\n",
    "    plt.legend(loc='best')\n",
    "    print('Saving:',folder + '/Lead_jet_' + str(key) + '.pdf')\n",
    "    plt.savefig(folder + \"/Lead_jet_\" + str(key) + '.pdf')\n",
    "  for key in SubLeadJet.keys() :\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "    fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "    bins = np.linspace(my_max(min(SubLeadJet[key]),-99.), max(SubLeadJet[key]), 100)\n",
    "    _ = plt.hist(SubLeadJet[key],  bins=bins, histtype='step', normed=True, label=r'$SubLeading Jet$', linewidth=2)\n",
    "    plt.xlabel(key)\n",
    "    plt.ylabel('Entries')\n",
    "    plt.legend(loc='best')\n",
    "    print('Saving:',folder + '/SubLead_jet_' + str(key) + '.pdf')\n",
    "    plt.savefig(folder + \"/SubLead_jet_\" + str(key) + '.pdf')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets look at the correlations of the features\n",
    "c1 = ROOT.TCanvas(); c1.cd(); ROOT.gStyle.SetOptStat(0)\n",
    "if MakePlots:\n",
    "    h_Corr_Lead    = ROOT.TH2F(\"h_Corr_Lead\",\"\", len(my_branches_training_jet1), 0, len(my_branches_training_jet1), len(my_branches_training_jet1), 0, len(my_branches_training_jet1))\n",
    "    h_Corr_SubLead = ROOT.TH2F(\"h_Corr_SubLead\",\"\", len(my_branches_training_jet2), 0, len(my_branches_training_jet2), len(my_branches_training_jet2), 0, len(my_branches_training_jet2))\n",
    "    for var1 in range(len(my_branches_training_jet1)):\n",
    "        h_Corr_Lead.GetXaxis().SetBinLabel(var1+1,my_branches_training_jet1[var1])\n",
    "        h_Corr_SubLead.GetXaxis().SetBinLabel(var1+1,my_branches_training_jet2[var1])\n",
    "        for var2 in range(len(my_branches_training_jet1)):\n",
    "            h_Corr_Lead.GetYaxis().SetBinLabel(var2+1,my_branches_training_jet1[var2])\n",
    "            h_Corr_SubLead.GetYaxis().SetBinLabel(var2+1,my_branches_training_jet2[var2])\n",
    "            if(var2>=var1):\n",
    "                array_Var1_Lead_var1    = np.array( LeadJet[my_branches_training_jet1[var1]] )\n",
    "                array_Var1_Lead_var2    = np.array( LeadJet[my_branches_training_jet1[var2]] )\n",
    "                array_Var1_SubLead_var1 = np.array( SubLeadJet[my_branches_training_jet2[var1]] )\n",
    "                array_Var1_SubLead_var2 = np.array( SubLeadJet[my_branches_training_jet2[var2]] )\n",
    "                corr = scipy.stats.pearsonr( array_Var1_Lead_var1, array_Var1_Lead_var2 )[0]\n",
    "                h_Corr_Lead.SetBinContent(var1+1,var2+1,corr)\n",
    "                corr = scipy.stats.pearsonr( array_Var1_SubLead_var1, array_Var1_SubLead_var2 )[0]\n",
    "                h_Corr_SubLead.SetBinContent(var1+1,var2+1,corr)\n",
    "    h_Corr_Lead.GetZaxis().SetRangeUser(-1.,1.)\n",
    "    h_Corr_SubLead.GetZaxis().SetRangeUser(-1.,1.)\n",
    "    ROOT.gStyle.SetPaintTextFormat(\".2f\");\n",
    "    h_Corr_Lead.Draw(\"colzTEXT\")\n",
    "    c1.SaveAs(folder + '/Corr_hh.pdf')\n",
    "    h_Corr_SubLead.Draw(\"colzTEXT\")\n",
    "    c1.SaveAs(folder + '/Corr_ttbar.pdf')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need:\n",
    "1. X : matrix with raw=#Events and column=Variables to discriminate.\"\n",
    "2. w : A vector containig the weights of each event\"\n",
    "3. Y : A vector containing for each event if it is signal (0) or TT (1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now lets start to talk about DNN!\n"
     ]
    }
   ],
   "source": [
    "print('Now lets start to talk about DNN!')\n",
    "# Turn the df the desired ndarray \"X\" that can be directly used for ML applications.\n",
    "X    = LeadJet[my_branches_training_jet1].as_matrix() # Each row is an object to classify, each column corresponds to a specific variable.\n",
    "if (doSubLead):\n",
    "    X = SubLeadJet[my_branches_training_jet2].as_matrix()\n",
    "# No weights needed, just set al to 1\n",
    "w    =  np.ones(X.shape[0])\n",
    "# This is the array with the true values: 0 is signal, 1 if TT.\n",
    "y = LeadJet[\"target\"]\n",
    "if (doSubLead):\n",
    "    y = SubLeadJet[\"target\"]\n",
    "\n",
    "# Randomly shuffle and automatically split all your objects into train and test subsets\n",
    "ix = range(X.shape[0]) # array of indices, just to keep track of them for safety reasons and future checks\n",
    "X_train, X_test, y_train, y_test, w_train, w_test, ix_train, ix_test = train_test_split(X, y, w, ix, train_size=0.7) # Train here is 70% of the total statistic\n",
    "# It is common practice to scale the inputs to Neural Nets such that they have approximately similar ranges (it atually improve the results)\n",
    "scaler = StandardScaler() \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test) # You are applying the same transformation done to X_train, to X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a while, but it is worth do do it once. It shows the correlations for S and B overimposed.\n",
    "# More variables you add in var_toPlot, more times it takes.\n",
    "if MakePlots:\n",
    "    # Next lines to select the same number of entries\n",
    "    var_toPlot = [\"target\",\"numOfVertices\",\"b1jet_pt\",\"b1jet_eta\",\"b1jet_mt\",\"b1jet_leadTrackPt\",\"b1jet_leptonDeltaR\",\"b1jet_leptonPtRel\",\"b1jet_leptonPt\",\"b1jet_vtxPt\",\"b1jet_vtxMass\",\"b1jet_vtxNtracks\",\"b1jet_neHEF\",\"b1jet_neEmEF\",\"b1jet_vtx3DSig\",\"b1jet_vtx3DVal\",\"b1genjet_pt\"]\n",
    "    if (doSubLead):\n",
    "        var_toPlot = [\"target\",\"numOfVertices\",\"b2jet_pt\",\"b2jet_eta\",\"b2jet_mt\",\"b2jet_leadTrackPt\",\"b2jet_leptonDeltaR\",\"b2jet_leptonPtRel\",\"b2jet_leptonPt\",\"b2jet_vtxPt\",\"b2jet_vtxMass\",\"b2jet_vtxNtracks\",\"b2jet_neHEF\",\"b2jet_neEmEF\",\"b2jet_vtx3DSig\",\"b2jet_vtx3DVal\",\"b2genjet_pt\"]\n",
    "\n",
    "    # You can select the variable to plot in sns.pairplot using an argumnet vars=['var1','var2'...]\n",
    "    sns_plot = sns.pairplot(LeadJet,palette=[\"#e74c3c\"],plot_kws={\"s\": 3,\"alpha\":0.3},size=5)\n",
    "    sns_plot.savefig(folder + \"/Variables_pairplot_LeadingJet.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13cb063d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fitting the Train sample to get our model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 10)                170       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,261\n",
      "Trainable params: 1,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=X_train.shape[1], activation='relu')) # Linear transformation of the input vector. The first number is output_dim.\\n\",\n",
    "    model.add(Dropout(0.1)) # To avoid overfitting. It masks the outputs of the previous layer such that some of them will randomly become inactive and will not contribute to infor\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(20, activation='sigmoid'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(10,activation='tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(20, activation='sigmoid'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=15, verbose=0)\n",
    "print \"Now fitting the Train sample to get our model\"\n",
    "estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAAKvCAYAAACcZBf7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13cd249d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.0305040787649\n",
      "161     0.267562\n",
      "788     0.982029\n",
      "1817    0.461133\n",
      "1049    0.131414\n",
      "1949    0.971552\n",
      "4113    0.610395\n",
      "2099    0.858617\n",
      "1310    0.960025\n",
      "260     1.008307\n",
      "2498    0.439508\n",
      "3705    0.457651\n",
      "1684    0.812769\n",
      "623     0.978718\n",
      "2597    1.079117\n",
      "3481    1.136976\n",
      "1094    0.680115\n",
      "2378    1.025760\n",
      "3576    1.040268\n",
      "1148    0.870758\n",
      "3115    0.961922\n",
      "1050    0.922203\n",
      "1512    1.120676\n",
      "1680    0.613721\n",
      "1141    0.848342\n",
      "1947    1.193820\n",
      "4089    0.897105\n",
      "1759    0.552366\n",
      "773     0.811911\n",
      "3687    1.072908\n",
      "447     0.566300\n",
      "          ...   \n",
      "3723    0.651700\n",
      "1507    0.931943\n",
      "363     1.031066\n",
      "1877    0.797352\n",
      "3148    0.303738\n",
      "2138    0.832334\n",
      "1667    0.965613\n",
      "3670    0.969942\n",
      "3389    1.076624\n",
      "2720    0.119696\n",
      "3802    0.636623\n",
      "2794    0.931069\n",
      "2885    1.309619\n",
      "142     0.416600\n",
      "863     0.807944\n",
      "3890    0.839902\n",
      "1906    0.173783\n",
      "1220    0.952393\n",
      "670     0.792081\n",
      "3397    0.214364\n",
      "2196    1.095870\n",
      "3362    0.904996\n",
      "2891    1.035739\n",
      "2273    0.637218\n",
      "3207    0.969339\n",
      "2742    0.915351\n",
      "4174    1.133342\n",
      "4105    0.209304\n",
      "3145    0.164623\n",
      "2800    0.969154\n",
      "Name: target, dtype: float32\n",
      "('Saving:', 'Plots_Regression//Regression_PtRes.pdf')\n"
     ]
    }
   ],
   "source": [
    "# Get a prediction on the test\n",
    "y_hat = estimator.predict(X_test)\n",
    "\n",
    "# Get the score\n",
    "score = estimator.score(X_test, y_test)\n",
    "print \"score:\", score\n",
    "\n",
    "# Plot the test target and the estimated one\n",
    "%matplotlib inline\n",
    "if MakePlots:\n",
    "    # Estimatd target\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "    fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "    bins = np.linspace(my_max(min(y_hat),0.), max(y_hat), 100)\n",
    "    _ = plt.hist(y_hat,  bins=bins, histtype='step', normed=True, label=r'$y_hat$', linewidth=2)\n",
    "    plt.xlabel(\"y_hat\")\n",
    "    plt.ylabel('Entries')\n",
    "    plt.legend(loc='best')\n",
    "    print('Saving:',folder + '/Regression_y_hat.pdf')\n",
    "    plt.savefig(folder + '/Regression_y_hat.pdf')\n",
    "    # Estimatd target/ True target\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "    fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "    bins = np.linspace(my_max(min(y_hat/y_test),0.), 2, 100)\n",
    "    _ = plt.hist(y_hat/y_test,  bins=bins, histtype='step', normed=True, label=r'$y_hat/y$', linewidth=2)\n",
    "    plt.xlabel(\"y_hat/y\")\n",
    "    plt.ylabel('Entries')\n",
    "    plt.legend(loc='best')\n",
    "    print('Saving:',folder + '/Regression_y_hat_OverY.pdf')\n",
    "    plt.savefig(folder + '/Regression_y_hat_OverY.pdf')\n",
    "if True:\n",
    "    print y_test\n",
    "    # Pt Resolution Before After (first I have to find the index in X_test that corresponf to the Jet and Genjet pT)\n",
    "    N_bjet_pt   = -1; N_bgenjet_pt = -1\n",
    "    N_tmp = 0\n",
    "    for Feature in my_branches_training_jet1:\n",
    "        if(Feature==\"b1jet_pt\"): N_bjet_pt = N_tmp\n",
    "        if(Feature==\"b1genjet_pt\"): N_bgenjet_pt = N_tmp\n",
    "        N_tmp += 1\n",
    "    bjet_pt = X_test[:,N_bjet_pt]\n",
    "    bgenjet_pt = X_test[:,N_bgenjet_pt]\n",
    "    Variable       = (bjet_pt-bgenjet_pt)       \n",
    "    Variable_corr  = (bjet_pt/y_hat-bgenjet_pt) \n",
    "    Variable_ideal = (bjet_pt/y_test-bgenjet_pt)\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "    fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "    bins = np.linspace(-5, 5, 100)\n",
    "    _ = plt.hist(Variable,  bins=bins, histtype='step', normed=True, label=r'$STD$', linewidth=2)\n",
    "    _ = plt.hist(Variable_corr,  bins=bins, histtype='step', normed=True, label=r'$CORR$', linewidth=2)\n",
    "    _ = plt.hist(Variable_ideal,  bins=bins, histtype='step', normed=True, label=r'$IDEAL$', linewidth=2)\n",
    "    plt.xlabel(\"pT Resolution\")\n",
    "    plt.ylabel('Entries')\n",
    "    plt.legend(loc='best')\n",
    "    print('Saving:',folder + '/Regression_PtRes.pdf')\n",
    "    plt.savefig(folder + '/Regression_PtRes.pdf')\n",
    "\n",
    "#kfold = KFold(n_splits=10, random_state=seed)\n",
    "#results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "#print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a92610a00657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the best network (by default you return the last one, you if you save every time you have a better one you are fine loading it later)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./models/tutorial-progress.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Saving weights...'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./models/tutorial.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mjson_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the best network (by default you return the last one, you if you save every time you have a better one you are fine loading it later)\n",
    "model.load_weights('./models/tutorial-progress.h5')\n",
    "print 'Saving weights...'\n",
    "model.save_weights('./models/tutorial.h5', overwrite=True)\n",
    "json_string = model.to_json()\n",
    "open('./models/tutorial.json', 'w').write(json_string)\n",
    "print 'Testing...'\n",
    "yhat = model.predict(X_test, verbose = True, batch_size = 50) # Return a vector of 2 indeces [probToBe_S,probToBe_B]\n",
    "#Turn them into classes\n",
    "yhat_cls = np.argmax(yhat, axis=1) # Transform [probToBe_S,probToBe_B] in a vector of 0 and 1 depending if probToBe_S>probToBe_B. Practically return the index of the biggest element (0 is is probToBe_S, if is probToBe_B)\n",
    "# This should Normalized to the Xsec?\n",
    "if MakePlots:\n",
    "    bins = np.linspace(-0.5,1.5,3)\n",
    "    names = ['','','','hh','','','','tt']\n",
    "    fig = plt.figure(figsize=(11.69, 8.27), dpi=100)\n",
    "    ax = plt.subplot()\n",
    "    ax.set_xticklabels(names, rotation=45)\n",
    "    _ = plt.hist(yhat_cls, bins=bins, histtype='stepfilled', alpha=0.5, label='prediction',log=True)#, weights=w_test)\n",
    "    _ = plt.hist(y_test, bins=bins, histtype='stepfilled', alpha=0.5, label='truth',log=True)#, weights=w_test)\n",
    "    plt.legend(loc='upper right')\n",
    "    print('Saving:',folder + '/Performance.pdf')\n",
    "    plt.savefig(folder + '/Performance.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With \"(y_test != 0) & (yhat_cls == 0)\" you get an arrate of bool: [False False False ..., False False False]\n",
    "print 'Signal efficiency:',     w_test[(y_test == 0) & (yhat_cls == 0)].sum() / w_test[y_test == 0].sum()\n",
    "print 'Background efficiency:', w_test[(y_test != 0) & (yhat_cls == 0)].sum() / w_test[y_test != 0].sum()\n",
    "w_1 = np.ones(len(y_test))\n",
    "print 'Signal efficiency (not weighted):',     w_1[(y_test == 0) & (yhat_cls == 0)].sum() / w_1[y_test == 0].sum()\n",
    "print 'Background efficiency (not weighted):', w_1[(y_test != 0) & (yhat_cls == 0)].sum() / w_1[y_test != 0].sum()\n",
    "print \"Let's compare with the training samples:\"\n",
    "w_1 = np.ones(len(y_train))\n",
    "yhat_tr = model.predict(X_train, verbose = True, batch_size = 50)\n",
    "yhat_trcls = np.argmax(yhat_tr, axis=1)\n",
    "print ''; print 'Signal efficiency (not weighted) for training:',     w_1[(y_train == 0) & (yhat_trcls == 0)].sum() / w_1[y_train == 0].sum()\n",
    "print 'Background efficiency (not weighted) for training:', w_1[(y_train != 0) & (yhat_trcls == 0)].sum() / w_1[y_train != 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
